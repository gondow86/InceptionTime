{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from tsai.imports import *\n",
    "from tsai.models.layers import *\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy import signal\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 12\n",
    "close_num = 12\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 300\n",
    "sequence_len = 1000 * 5 # sampling_rate * second\n",
    "overlap = int(sequence_len * 0.3)\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "scaler = MinMaxScaler((0, 1))\n",
    "\n",
    "for i in range(1, num_class + 1):\n",
    "    file_path_fil = \"./data/BPF/filterd%02d.csv\" % i\n",
    "    file_path_unfil = \"./data/BPF/unfilterd%02d.csv\" % i\n",
    "    data_fil = pd.read_csv(file_path_fil)\n",
    "    data_np_fil = data_fil.to_numpy().flatten()\n",
    "    # df_fil = pd.DataFrame(data_fil)\n",
    "    # data_unfil = pd.read_csv(file_path_unfil)\n",
    "    # df_unfil = pd.DataFrame(data_unfil)\n",
    "\n",
    "    end = len(data_np_fil)\n",
    "    n = 0\n",
    "    n_stop = sequence_len\n",
    "    data_segs = []\n",
    "    while n_stop < end:\n",
    "        n_start = 0 + ((sequence_len - 1) - (overlap - 1)) * n\n",
    "        n_stop = n_start + sequence_len\n",
    "        tmp = []\n",
    "        seg = data_np_fil[n_start:n_stop].copy()\n",
    "        data_segs.append([seg])\n",
    "        n += 1\n",
    "\n",
    "    data_list.append(data_segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.37779185, 0.37960583, 0.38139324, ..., 0.00149465, 0.00150279,\n",
       "        0.00151537])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[4][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i in range(len(data_list)):\n",
    "    for j in range(len(data_list[i])):\n",
    "        if i <= close_num:\n",
    "            labels.append(i)\n",
    "        else:\n",
    "            labels.append(close_num + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3747/2521482540.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  data_tensor_list = torch.tensor(data_series_list)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([3657, 1, 5000]), torch.Size([3657]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df_list = pd.DataFrame(data_list)\n",
    "tmp = data_df_list.to_numpy().flatten().copy()\n",
    "data_series_list = pd.Series(tmp).dropna()\n",
    "data_np_list = data_series_list.to_numpy().flatten() # tmp => data_np_list\n",
    "labels_np = np.array(labels)\n",
    "for i in reversed(range(len(data_np_list))):\n",
    "    if len(data_np_list[i][0]) != sequence_len: # 決まった長さでないといけない\n",
    "        data_np_list = np.delete(data_np_list, i)\n",
    "        labels_np = np.delete(labels_np, i)\n",
    "data_series_list = pd.Series(data_np_list)\n",
    "labels_series = pd.Series(labels_np)\n",
    "data_tensor_list = torch.tensor(data_series_list)\n",
    "labels_tensor = torch.tensor(labels_series)\n",
    "data_tensor_list.shape, labels_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionModule(Module):\n",
    "    def __init__(self, ni, nf, ks=40, bottleneck=True):\n",
    "        ks = [ks // (2**i) for i in range(3)]\n",
    "        ks = [k if k % 2 != 0 else k - 1 for k in ks]  # ensure odd ks\n",
    "        bottleneck = bottleneck if ni > 1 else False\n",
    "        self.bottleneck = Conv1d(ni, nf, 1, bias=False) if bottleneck else noop\n",
    "        self.convs = nn.ModuleList([Conv1d(nf if bottleneck else ni, nf, k, bias=False) for k in ks])\n",
    "        self.maxconvpool = nn.Sequential(*[nn.MaxPool1d(3, stride=1, padding=1), Conv1d(ni, nf, 1, bias=False)])\n",
    "        self.concat = Concat()\n",
    "        self.bn = BN1d(nf * 4)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_tensor = x\n",
    "        x = self.bottleneck(input_tensor)\n",
    "        x = self.concat([l(x) for l in self.convs] + [self.maxconvpool(input_tensor)])\n",
    "        return self.act(self.bn(x))\n",
    "\n",
    "\n",
    "@delegates(InceptionModule.__init__)\n",
    "class InceptionBlock(Module):\n",
    "    def __init__(self, ni, nf=32, residual=True, depth=6, **kwargs):\n",
    "        self.residual, self.depth = residual, depth\n",
    "        self.inception, self.shortcut = nn.ModuleList(), nn.ModuleList()\n",
    "        for d in range(depth):\n",
    "            self.inception.append(InceptionModule(ni if d == 0 else nf * 4, nf, **kwargs))\n",
    "            if self.residual and d % 3 == 2: \n",
    "                n_in, n_out = ni if d == 2 else nf * 4, nf * 4\n",
    "                self.shortcut.append(BN1d(n_in) if n_in == n_out else ConvBlock(n_in, n_out, 1, act=None))\n",
    "        self.add = Add()\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        for d, l in enumerate(range(self.depth)):\n",
    "            x = self.inception[d](x)\n",
    "            if self.residual and d % 3 == 2: res = x = self.act(self.add(x, self.shortcut[d//3](res)))\n",
    "        return x\n",
    "\n",
    "    \n",
    "@delegates(InceptionModule.__init__)\n",
    "class InceptionTime(Module):\n",
    "    def __init__(self, c_in, c_out, seq_len=None, nf=32, nb_filters=None, **kwargs):\n",
    "        nf = ifnone(nf, nb_filters) # for compatibility\n",
    "        self.inceptionblock = InceptionBlock(c_in, nf, **kwargs) # c_in is input channel num of conv1d\n",
    "        self.gap = GAP1d(1)\n",
    "        self.fc = nn.Linear(nf * 4, c_out) # c_out is 1d output size \n",
    "        self.fc_tsne = nn.Linear(nf * 4, 2)\n",
    "        self.two_vecs_train = [] # list is faster in appending\n",
    "        self.two_vecs_test = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.inceptionblock(x)\n",
    "        x = self.gap(x)\n",
    "        two_dimensional_vec = self.fc_tsne(x)\n",
    "        if self.training:\n",
    "            self.two_vecs_train.append(two_dimensional_vec.tolist()) # あとでネストしたものをまとめてtensorかndarrayに変換するため\n",
    "        else:\n",
    "            self.two_vecs_test.append(two_dimensional_vec.tolist())\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HCU_Dataset(Dataset):\n",
    "    def __init__(self, dataset, labels) -> None:\n",
    "        # super().__init__()\n",
    "        self.radar_heartbeat = dataset\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "          idx = idx.tolist()        \n",
    "        return self.radar_heartbeat[idx], self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.radar_heartbeat)\n",
    "\n",
    "\n",
    "dataset = HCU_Dataset(data_tensor_list, labels_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットでラベルが変わる区切りを発見する(openset用)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0105, 0.0106, 0.0106,  ..., 0.0023, 0.0022, 0.0022]],\n",
       "        dtype=torch.float64),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1000]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### close setの場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set: 2925, Test Set: 732\n"
     ]
    }
   ],
   "source": [
    "close_train_size = int(0.80 * len(dataset))\n",
    "close_test_size = len(dataset) - close_train_size\n",
    "close_train_set, close_test_set = torch.utils.data.random_split(dataset, [close_train_size, close_test_size])\n",
    "print(f\"Train Set: {len(close_train_set)}, Test Set: {len(close_test_set)}\")\n",
    "train_loader = DataLoader(dataset=close_train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=close_test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import log\n",
    "softmax = nn.Softmax()\n",
    "\n",
    "def softmax_loss(outputs, labels):\n",
    "    loss = 0\n",
    "    batch_size = len(labels)\n",
    "    logsoftmax_out = log(softmax(outputs))\n",
    "    for idx in range(batch_size):\n",
    "        loss += 1.0 - logsoftmax_out[idx][labels[idx]]\n",
    "    \n",
    "    return loss / batch_size\n",
    "\n",
    "\n",
    "from center_loss import CenterLoss\n",
    "center_loss = CenterLoss(num_classes=close_num + 1, feat_dim=close_num + 1, use_gpu=True) # 入出力が同じだと一見変な感じがするが，交差エントロピーと違ってcenterlossを使うと最初から決めていれば，モデルの出力サイズを必ずしもクラス数に一致させる必要がないからfeat_dimを任意に設定できる．\n",
    "optimizer_centloss = torch.optim.SGD(center_loss.parameters(), lr=0.05)\n",
    "\n",
    "\n",
    "class AngularPenaltySMLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, loss_type='cosface', eps=1e-7, s=None, m=None):\n",
    "        '''\n",
    "        Angular Penalty Softmax Loss\n",
    "        Three 'loss_types' available: ['arcface', 'sphereface', 'cosface']\n",
    "        These losses are described in the following papers: \n",
    "        \n",
    "        ArcFace: https://arxiv.org/abs/1801.07698\n",
    "        SphereFace: https://arxiv.org/abs/1704.08063\n",
    "        CosFace/Ad Margin: https://arxiv.org/abs/1801.05599\n",
    "        '''\n",
    "        super(AngularPenaltySMLoss, self).__init__()\n",
    "        loss_type = loss_type.lower()\n",
    "        assert loss_type in  ['arcface', 'sphereface', 'cosface']\n",
    "        if loss_type == 'arcface':\n",
    "            self.s = 64.0 if not s else s\n",
    "            self.m = 0.5 if not m else m\n",
    "        if loss_type == 'sphereface':\n",
    "            self.s = 64.0 if not s else s\n",
    "            self.m = 1.35 if not m else m\n",
    "        if loss_type == 'cosface':\n",
    "            self.s = 32.0 if not s else s\n",
    "            self.m = 0.2 if not m else m\n",
    "        self.loss_type = loss_type\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.fc = nn.Linear(in_features, out_features, bias=False)\n",
    "        self.fc.to(device)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        '''\n",
    "        input shape (N, in_features)\n",
    "        '''\n",
    "        assert len(x) == len(labels)\n",
    "        assert torch.min(labels) >= 0\n",
    "        assert torch.max(labels) < self.out_features\n",
    "        \n",
    "        for W in self.fc.parameters():\n",
    "            W = F.normalize(W, p=2, dim=1)\n",
    "\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        # logを引き算に変えて計算\n",
    "        wf = self.fc(x)\n",
    "        if self.loss_type == 'cosface':\n",
    "            numerator = self.s * (torch.diagonal(wf.transpose(0, 1)[labels]) - self.m)\n",
    "        if self.loss_type == 'arcface':\n",
    "            numerator = self.s * torch.cos(torch.acos(torch.clamp(torch.diagonal(wf.transpose(0, 1)[labels]), -1.+self.eps, 1-self.eps)) + self.m)\n",
    "        if self.loss_type == 'sphereface':\n",
    "            numerator = self.s * torch.cos(self.m * torch.acos(torch.clamp(torch.diagonal(wf.transpose(0, 1)[labels]), -1.+self.eps, 1-self.eps)))\n",
    "\n",
    "        excl = torch.cat([torch.cat((wf[i, :y], wf[i, y+1:])).unsqueeze(0) for i, y in enumerate(labels)], dim=0)\n",
    "        denominator = torch.exp(numerator) + torch.sum(torch.exp(self.s * excl), dim=1)\n",
    "        L = numerator - torch.log(denominator)\n",
    "        return -torch.mean(L)\n",
    "\n",
    "cos_loss = AngularPenaltySMLoss(close_num + 1, close_num + 1, loss_type=\"cosface\") # center_lossと同じ理由でin_featuresはクラス数でよい．\n",
    "\n",
    "\n",
    "def triple_joint_loss(output, label, alpha):\n",
    "    # alpha: hyper parameter\n",
    "    output_only_truth = []\n",
    "    for idx, x in enumerate(output):\n",
    "        x = x[labels[idx]]\n",
    "        x = torch.tensor(x).to(device)\n",
    "        output_only_truth.append([x])\n",
    "    output_only_truth = torch.tensor(output_only_truth)\n",
    "    output_only_truth = output_only_truth.float()\n",
    "    output_only_truth = output_only_truth.to(device)\n",
    "    # print(output.is_cuda, output_only_truth.is_cuda, label.is_cuda)\n",
    "\n",
    "    return softmax_loss(output, label) + alpha * center_loss(output, label)\n",
    "    # return softmax_loss(output, label) + alpha * center_loss(output, label) + cos_loss(output, label)\n",
    "    # return cos_loss(output_only_truth, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3747/3433469541.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x).to(device)\n",
      "/mnt/c/Users/grpro/workspace/grad_thesis/InceptionTime/env/lib/python3.9/site-packages/torch/overrides.py:1498: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  result = torch_func_method(public_api, types, args, kwargs)\n",
      "/mnt/c/Users/grpro/workspace/grad_thesis/InceptionTime/center_loss.py:45: UserWarning: This overload of addmm_ is deprecated:\n",
      "\taddmm_(Number beta, Number alpha, Tensor mat1, Tensor mat2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddmm_(Tensor mat1, Tensor mat2, *, Number beta, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1174.)\n",
      "  distmat.addmm_(1, -2, x, self.centers.t())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/`300], Step [10/46], Loss: 4.3609\n",
      "Epoch [1/`300], Step [20/46], Loss: 4.1960\n",
      "Epoch [1/`300], Step [30/46], Loss: 4.0504\n",
      "Epoch [1/`300], Step [40/46], Loss: 3.9615\n",
      "Epoch [2/`300], Step [10/46], Loss: 3.8900\n",
      "Epoch [2/`300], Step [20/46], Loss: 3.7113\n",
      "Epoch [2/`300], Step [30/46], Loss: 3.8807\n",
      "Epoch [2/`300], Step [40/46], Loss: 3.8141\n",
      "Epoch [3/`300], Step [10/46], Loss: 3.7848\n",
      "Epoch [3/`300], Step [20/46], Loss: 3.5240\n",
      "Epoch [3/`300], Step [30/46], Loss: 3.4466\n",
      "Epoch [3/`300], Step [40/46], Loss: 3.5442\n",
      "Epoch [4/`300], Step [10/46], Loss: 3.3910\n",
      "Epoch [4/`300], Step [20/46], Loss: 3.4229\n",
      "Epoch [4/`300], Step [30/46], Loss: 3.3973\n",
      "Epoch [4/`300], Step [40/46], Loss: 3.4593\n",
      "Epoch [5/`300], Step [10/46], Loss: 3.3272\n",
      "Epoch [5/`300], Step [20/46], Loss: 3.4672\n",
      "Epoch [5/`300], Step [30/46], Loss: 3.3221\n",
      "Epoch [5/`300], Step [40/46], Loss: 3.5919\n",
      "Epoch [6/`300], Step [10/46], Loss: 3.3637\n",
      "Epoch [6/`300], Step [20/46], Loss: 3.1696\n",
      "Epoch [6/`300], Step [30/46], Loss: 3.4400\n",
      "Epoch [6/`300], Step [40/46], Loss: 3.3420\n",
      "Epoch [7/`300], Step [10/46], Loss: 3.2678\n",
      "Epoch [7/`300], Step [20/46], Loss: 3.1921\n",
      "Epoch [7/`300], Step [30/46], Loss: 3.2883\n",
      "Epoch [7/`300], Step [40/46], Loss: 3.2858\n",
      "Epoch [8/`300], Step [10/46], Loss: 3.2421\n",
      "Epoch [8/`300], Step [20/46], Loss: 3.2784\n",
      "Epoch [8/`300], Step [30/46], Loss: 3.3854\n",
      "Epoch [8/`300], Step [40/46], Loss: 3.4133\n",
      "Epoch [9/`300], Step [10/46], Loss: 3.2182\n",
      "Epoch [9/`300], Step [20/46], Loss: 3.4558\n",
      "Epoch [9/`300], Step [30/46], Loss: 3.3708\n",
      "Epoch [9/`300], Step [40/46], Loss: 3.3916\n",
      "Epoch [10/`300], Step [10/46], Loss: 3.2905\n",
      "Epoch [10/`300], Step [20/46], Loss: 3.1626\n",
      "Epoch [10/`300], Step [30/46], Loss: 3.3211\n",
      "Epoch [10/`300], Step [40/46], Loss: 3.3216\n",
      "Epoch [11/`300], Step [10/46], Loss: 3.1671\n",
      "Epoch [11/`300], Step [20/46], Loss: 3.3000\n",
      "Epoch [11/`300], Step [30/46], Loss: 3.3388\n",
      "Epoch [11/`300], Step [40/46], Loss: 3.2337\n",
      "Epoch [12/`300], Step [10/46], Loss: 3.2636\n",
      "Epoch [12/`300], Step [20/46], Loss: 3.1090\n",
      "Epoch [12/`300], Step [30/46], Loss: 3.2258\n",
      "Epoch [12/`300], Step [40/46], Loss: 3.2999\n",
      "Epoch [13/`300], Step [10/46], Loss: 3.1635\n",
      "Epoch [13/`300], Step [20/46], Loss: 3.2497\n",
      "Epoch [13/`300], Step [30/46], Loss: 3.1795\n",
      "Epoch [13/`300], Step [40/46], Loss: 3.2975\n",
      "Epoch [14/`300], Step [10/46], Loss: 3.2350\n",
      "Epoch [14/`300], Step [20/46], Loss: 3.2585\n",
      "Epoch [14/`300], Step [30/46], Loss: 3.2285\n",
      "Epoch [14/`300], Step [40/46], Loss: 3.2922\n",
      "Epoch [15/`300], Step [10/46], Loss: 3.3437\n",
      "Epoch [15/`300], Step [20/46], Loss: 3.3095\n",
      "Epoch [15/`300], Step [30/46], Loss: 3.2723\n",
      "Epoch [15/`300], Step [40/46], Loss: 3.2593\n",
      "Epoch [16/`300], Step [10/46], Loss: 3.2487\n",
      "Epoch [16/`300], Step [20/46], Loss: 3.3330\n",
      "Epoch [16/`300], Step [30/46], Loss: 3.2748\n",
      "Epoch [16/`300], Step [40/46], Loss: 3.0923\n",
      "Epoch [17/`300], Step [10/46], Loss: 3.2605\n",
      "Epoch [17/`300], Step [20/46], Loss: 3.1796\n",
      "Epoch [17/`300], Step [30/46], Loss: 3.2947\n",
      "Epoch [17/`300], Step [40/46], Loss: 3.1111\n",
      "Epoch [18/`300], Step [10/46], Loss: 3.2034\n",
      "Epoch [18/`300], Step [20/46], Loss: 3.3386\n",
      "Epoch [18/`300], Step [30/46], Loss: 3.2463\n",
      "Epoch [18/`300], Step [40/46], Loss: 3.0525\n",
      "Epoch [19/`300], Step [10/46], Loss: 3.2930\n",
      "Epoch [19/`300], Step [20/46], Loss: 3.2715\n",
      "Epoch [19/`300], Step [30/46], Loss: 3.1927\n",
      "Epoch [19/`300], Step [40/46], Loss: 3.3466\n",
      "Epoch [20/`300], Step [10/46], Loss: 3.2001\n",
      "Epoch [20/`300], Step [20/46], Loss: 3.3365\n",
      "Epoch [20/`300], Step [30/46], Loss: 3.1980\n",
      "Epoch [20/`300], Step [40/46], Loss: 3.1944\n",
      "Epoch [21/`300], Step [10/46], Loss: 3.2825\n",
      "Epoch [21/`300], Step [20/46], Loss: 3.3274\n",
      "Epoch [21/`300], Step [30/46], Loss: 3.3433\n",
      "Epoch [21/`300], Step [40/46], Loss: 3.2007\n",
      "Epoch [22/`300], Step [10/46], Loss: 3.2417\n",
      "Epoch [22/`300], Step [20/46], Loss: 3.1911\n",
      "Epoch [22/`300], Step [30/46], Loss: 3.1569\n",
      "Epoch [22/`300], Step [40/46], Loss: 3.1876\n",
      "Epoch [23/`300], Step [10/46], Loss: 3.1776\n",
      "Epoch [23/`300], Step [20/46], Loss: 3.1978\n",
      "Epoch [23/`300], Step [30/46], Loss: 3.1366\n",
      "Epoch [23/`300], Step [40/46], Loss: 3.1469\n",
      "Epoch [24/`300], Step [10/46], Loss: 3.2281\n",
      "Epoch [24/`300], Step [20/46], Loss: 3.2080\n",
      "Epoch [24/`300], Step [30/46], Loss: 3.2502\n",
      "Epoch [24/`300], Step [40/46], Loss: 3.1052\n",
      "Epoch [25/`300], Step [10/46], Loss: 3.1915\n",
      "Epoch [25/`300], Step [20/46], Loss: 3.1956\n",
      "Epoch [25/`300], Step [30/46], Loss: 3.1623\n",
      "Epoch [25/`300], Step [40/46], Loss: 3.2525\n",
      "Epoch [26/`300], Step [10/46], Loss: 3.1519\n",
      "Epoch [26/`300], Step [20/46], Loss: 3.1861\n",
      "Epoch [26/`300], Step [30/46], Loss: 3.3410\n",
      "Epoch [26/`300], Step [40/46], Loss: 3.1252\n",
      "Epoch [27/`300], Step [10/46], Loss: 3.0879\n",
      "Epoch [27/`300], Step [20/46], Loss: 3.1431\n",
      "Epoch [27/`300], Step [30/46], Loss: 3.1886\n",
      "Epoch [27/`300], Step [40/46], Loss: 3.1967\n",
      "Epoch [28/`300], Step [10/46], Loss: 3.3004\n",
      "Epoch [28/`300], Step [20/46], Loss: 3.2182\n",
      "Epoch [28/`300], Step [30/46], Loss: 3.1700\n",
      "Epoch [28/`300], Step [40/46], Loss: 3.1051\n",
      "Epoch [29/`300], Step [10/46], Loss: 3.0806\n",
      "Epoch [29/`300], Step [20/46], Loss: 3.2236\n",
      "Epoch [29/`300], Step [30/46], Loss: 3.1238\n",
      "Epoch [29/`300], Step [40/46], Loss: 3.2812\n",
      "Epoch [30/`300], Step [10/46], Loss: 3.1428\n",
      "Epoch [30/`300], Step [20/46], Loss: 3.1350\n",
      "Epoch [30/`300], Step [30/46], Loss: 3.1851\n",
      "Epoch [30/`300], Step [40/46], Loss: 3.3742\n",
      "Epoch [31/`300], Step [10/46], Loss: 3.2055\n",
      "Epoch [31/`300], Step [20/46], Loss: 3.1746\n",
      "Epoch [31/`300], Step [30/46], Loss: 3.1338\n",
      "Epoch [31/`300], Step [40/46], Loss: 3.2102\n",
      "Epoch [32/`300], Step [10/46], Loss: 3.3386\n",
      "Epoch [32/`300], Step [20/46], Loss: 3.1224\n",
      "Epoch [32/`300], Step [30/46], Loss: 3.1712\n",
      "Epoch [32/`300], Step [40/46], Loss: 3.1838\n",
      "Epoch [33/`300], Step [10/46], Loss: 3.0481\n",
      "Epoch [33/`300], Step [20/46], Loss: 3.1695\n",
      "Epoch [33/`300], Step [30/46], Loss: 3.2029\n",
      "Epoch [33/`300], Step [40/46], Loss: 3.1568\n",
      "Epoch [34/`300], Step [10/46], Loss: 3.0736\n",
      "Epoch [34/`300], Step [20/46], Loss: 3.1119\n",
      "Epoch [34/`300], Step [30/46], Loss: 3.2641\n",
      "Epoch [34/`300], Step [40/46], Loss: 3.1155\n",
      "Epoch [35/`300], Step [10/46], Loss: 3.2142\n",
      "Epoch [35/`300], Step [20/46], Loss: 3.1259\n",
      "Epoch [35/`300], Step [30/46], Loss: 3.2092\n",
      "Epoch [35/`300], Step [40/46], Loss: 3.2165\n",
      "Epoch [36/`300], Step [10/46], Loss: 3.2064\n",
      "Epoch [36/`300], Step [20/46], Loss: 3.1502\n",
      "Epoch [36/`300], Step [30/46], Loss: 3.0887\n",
      "Epoch [36/`300], Step [40/46], Loss: 3.1312\n",
      "Epoch [37/`300], Step [10/46], Loss: 3.1126\n",
      "Epoch [37/`300], Step [20/46], Loss: 3.2057\n",
      "Epoch [37/`300], Step [30/46], Loss: 3.0580\n",
      "Epoch [37/`300], Step [40/46], Loss: 3.2879\n",
      "Epoch [38/`300], Step [10/46], Loss: 3.1279\n",
      "Epoch [38/`300], Step [20/46], Loss: 3.2496\n",
      "Epoch [38/`300], Step [30/46], Loss: 3.1737\n",
      "Epoch [38/`300], Step [40/46], Loss: 3.0974\n",
      "Epoch [39/`300], Step [10/46], Loss: 3.1615\n",
      "Epoch [39/`300], Step [20/46], Loss: 3.2943\n",
      "Epoch [39/`300], Step [30/46], Loss: 3.0797\n",
      "Epoch [39/`300], Step [40/46], Loss: 3.2235\n",
      "Epoch [40/`300], Step [10/46], Loss: 3.1452\n",
      "Epoch [40/`300], Step [20/46], Loss: 3.2024\n",
      "Epoch [40/`300], Step [30/46], Loss: 3.2399\n",
      "Epoch [40/`300], Step [40/46], Loss: 3.2465\n",
      "Epoch [41/`300], Step [10/46], Loss: 3.0881\n",
      "Epoch [41/`300], Step [20/46], Loss: 3.0463\n",
      "Epoch [41/`300], Step [30/46], Loss: 3.4057\n",
      "Epoch [41/`300], Step [40/46], Loss: 3.2119\n",
      "Epoch [42/`300], Step [10/46], Loss: 3.2478\n",
      "Epoch [42/`300], Step [20/46], Loss: 3.1941\n",
      "Epoch [42/`300], Step [30/46], Loss: 3.2401\n",
      "Epoch [42/`300], Step [40/46], Loss: 3.0235\n",
      "Epoch [43/`300], Step [10/46], Loss: 3.1231\n",
      "Epoch [43/`300], Step [20/46], Loss: 3.0825\n",
      "Epoch [43/`300], Step [30/46], Loss: 3.2304\n",
      "Epoch [43/`300], Step [40/46], Loss: 3.0654\n",
      "Epoch [44/`300], Step [10/46], Loss: 3.2050\n",
      "Epoch [44/`300], Step [20/46], Loss: 3.1246\n",
      "Epoch [44/`300], Step [30/46], Loss: 3.2322\n",
      "Epoch [44/`300], Step [40/46], Loss: 3.1751\n",
      "Epoch [45/`300], Step [10/46], Loss: 3.1272\n",
      "Epoch [45/`300], Step [20/46], Loss: 3.1077\n",
      "Epoch [45/`300], Step [30/46], Loss: 3.2471\n",
      "Epoch [45/`300], Step [40/46], Loss: 3.1754\n",
      "Epoch [46/`300], Step [10/46], Loss: 3.1297\n",
      "Epoch [46/`300], Step [20/46], Loss: 3.3604\n",
      "Epoch [46/`300], Step [30/46], Loss: 3.1611\n",
      "Epoch [46/`300], Step [40/46], Loss: 3.2279\n",
      "Epoch [47/`300], Step [10/46], Loss: 3.0630\n",
      "Epoch [47/`300], Step [20/46], Loss: 3.1337\n",
      "Epoch [47/`300], Step [30/46], Loss: 3.1720\n",
      "Epoch [47/`300], Step [40/46], Loss: 3.2096\n",
      "Epoch [48/`300], Step [10/46], Loss: 2.9616\n",
      "Epoch [48/`300], Step [20/46], Loss: 3.0455\n",
      "Epoch [48/`300], Step [30/46], Loss: 3.0594\n",
      "Epoch [48/`300], Step [40/46], Loss: 3.3234\n",
      "Epoch [49/`300], Step [10/46], Loss: 3.1127\n",
      "Epoch [49/`300], Step [20/46], Loss: 3.0838\n",
      "Epoch [49/`300], Step [30/46], Loss: 3.1944\n",
      "Epoch [49/`300], Step [40/46], Loss: 3.1496\n",
      "Epoch [50/`300], Step [10/46], Loss: 3.3577\n",
      "Epoch [50/`300], Step [20/46], Loss: 3.0448\n",
      "Epoch [50/`300], Step [30/46], Loss: 3.0807\n",
      "Epoch [50/`300], Step [40/46], Loss: 3.0307\n",
      "Epoch [51/`300], Step [10/46], Loss: 3.2061\n",
      "Epoch [51/`300], Step [20/46], Loss: 3.0765\n",
      "Epoch [51/`300], Step [30/46], Loss: 3.2109\n",
      "Epoch [51/`300], Step [40/46], Loss: 3.1329\n",
      "Epoch [52/`300], Step [10/46], Loss: 3.1525\n",
      "Epoch [52/`300], Step [20/46], Loss: 3.1331\n",
      "Epoch [52/`300], Step [30/46], Loss: 3.0253\n",
      "Epoch [52/`300], Step [40/46], Loss: 3.0119\n",
      "Epoch [53/`300], Step [10/46], Loss: 3.1978\n",
      "Epoch [53/`300], Step [20/46], Loss: 3.0272\n",
      "Epoch [53/`300], Step [30/46], Loss: 3.2709\n",
      "Epoch [53/`300], Step [40/46], Loss: 3.1702\n",
      "Epoch [54/`300], Step [10/46], Loss: 3.0226\n",
      "Epoch [54/`300], Step [20/46], Loss: 2.9941\n",
      "Epoch [54/`300], Step [30/46], Loss: 3.0291\n",
      "Epoch [54/`300], Step [40/46], Loss: 3.0730\n",
      "Epoch [55/`300], Step [10/46], Loss: 3.2228\n",
      "Epoch [55/`300], Step [20/46], Loss: 3.0646\n",
      "Epoch [55/`300], Step [30/46], Loss: 3.1372\n",
      "Epoch [55/`300], Step [40/46], Loss: 3.1536\n",
      "Epoch [56/`300], Step [10/46], Loss: 3.0733\n",
      "Epoch [56/`300], Step [20/46], Loss: 3.1955\n",
      "Epoch [56/`300], Step [30/46], Loss: 3.0819\n",
      "Epoch [56/`300], Step [40/46], Loss: 3.1345\n",
      "Epoch [57/`300], Step [10/46], Loss: 3.1881\n",
      "Epoch [57/`300], Step [20/46], Loss: 3.1093\n",
      "Epoch [57/`300], Step [30/46], Loss: 3.0609\n",
      "Epoch [57/`300], Step [40/46], Loss: 2.9450\n",
      "Epoch [58/`300], Step [10/46], Loss: 3.0127\n",
      "Epoch [58/`300], Step [20/46], Loss: 3.1539\n",
      "Epoch [58/`300], Step [30/46], Loss: 3.0655\n",
      "Epoch [58/`300], Step [40/46], Loss: 3.0809\n",
      "Epoch [59/`300], Step [10/46], Loss: 3.1854\n",
      "Epoch [59/`300], Step [20/46], Loss: 3.2455\n",
      "Epoch [59/`300], Step [30/46], Loss: 3.1360\n",
      "Epoch [59/`300], Step [40/46], Loss: 3.0670\n",
      "Epoch [60/`300], Step [10/46], Loss: 3.0236\n",
      "Epoch [60/`300], Step [20/46], Loss: 2.9972\n",
      "Epoch [60/`300], Step [30/46], Loss: 3.1022\n",
      "Epoch [60/`300], Step [40/46], Loss: 2.9980\n",
      "Epoch [61/`300], Step [10/46], Loss: 3.0499\n",
      "Epoch [61/`300], Step [20/46], Loss: 2.9744\n",
      "Epoch [61/`300], Step [30/46], Loss: 3.1053\n",
      "Epoch [61/`300], Step [40/46], Loss: 3.1111\n",
      "Epoch [62/`300], Step [10/46], Loss: 3.1405\n",
      "Epoch [62/`300], Step [20/46], Loss: 3.2098\n",
      "Epoch [62/`300], Step [30/46], Loss: 3.0526\n",
      "Epoch [62/`300], Step [40/46], Loss: 2.9289\n",
      "Epoch [63/`300], Step [10/46], Loss: 2.9717\n",
      "Epoch [63/`300], Step [20/46], Loss: 3.3560\n",
      "Epoch [63/`300], Step [30/46], Loss: 3.1139\n",
      "Epoch [63/`300], Step [40/46], Loss: 3.0514\n",
      "Epoch [64/`300], Step [10/46], Loss: 3.0696\n",
      "Epoch [64/`300], Step [20/46], Loss: 2.9219\n",
      "Epoch [64/`300], Step [30/46], Loss: 3.0487\n",
      "Epoch [64/`300], Step [40/46], Loss: 2.9951\n",
      "Epoch [65/`300], Step [10/46], Loss: 3.0616\n",
      "Epoch [65/`300], Step [20/46], Loss: 3.1292\n",
      "Epoch [65/`300], Step [30/46], Loss: 3.1755\n",
      "Epoch [65/`300], Step [40/46], Loss: 2.9612\n",
      "Epoch [66/`300], Step [10/46], Loss: 3.0538\n",
      "Epoch [66/`300], Step [20/46], Loss: 3.0108\n",
      "Epoch [66/`300], Step [30/46], Loss: 3.1787\n",
      "Epoch [66/`300], Step [40/46], Loss: 3.1647\n",
      "Epoch [67/`300], Step [10/46], Loss: 3.2392\n",
      "Epoch [67/`300], Step [20/46], Loss: 3.1083\n",
      "Epoch [67/`300], Step [30/46], Loss: 3.0137\n",
      "Epoch [67/`300], Step [40/46], Loss: 3.0130\n",
      "Epoch [68/`300], Step [10/46], Loss: 2.9992\n",
      "Epoch [68/`300], Step [20/46], Loss: 3.2554\n",
      "Epoch [68/`300], Step [30/46], Loss: 3.1039\n",
      "Epoch [68/`300], Step [40/46], Loss: 2.9910\n",
      "Epoch [69/`300], Step [10/46], Loss: 3.0616\n",
      "Epoch [69/`300], Step [20/46], Loss: 3.1653\n",
      "Epoch [69/`300], Step [30/46], Loss: 3.1070\n",
      "Epoch [69/`300], Step [40/46], Loss: 3.0882\n",
      "Epoch [70/`300], Step [10/46], Loss: 3.0006\n",
      "Epoch [70/`300], Step [20/46], Loss: 2.8775\n",
      "Epoch [70/`300], Step [30/46], Loss: 3.1920\n",
      "Epoch [70/`300], Step [40/46], Loss: 2.9697\n",
      "Epoch [71/`300], Step [10/46], Loss: 3.1220\n",
      "Epoch [71/`300], Step [20/46], Loss: 3.0977\n",
      "Epoch [71/`300], Step [30/46], Loss: 3.0229\n",
      "Epoch [71/`300], Step [40/46], Loss: 3.2353\n",
      "Epoch [72/`300], Step [10/46], Loss: 2.9687\n",
      "Epoch [72/`300], Step [20/46], Loss: 3.1658\n",
      "Epoch [72/`300], Step [30/46], Loss: 2.9457\n",
      "Epoch [72/`300], Step [40/46], Loss: 2.9182\n",
      "Epoch [73/`300], Step [10/46], Loss: 3.0695\n",
      "Epoch [73/`300], Step [20/46], Loss: 2.9688\n",
      "Epoch [73/`300], Step [30/46], Loss: 3.0248\n",
      "Epoch [73/`300], Step [40/46], Loss: 3.1278\n",
      "Epoch [74/`300], Step [10/46], Loss: 3.0156\n",
      "Epoch [74/`300], Step [20/46], Loss: 2.9408\n",
      "Epoch [74/`300], Step [30/46], Loss: 3.0698\n",
      "Epoch [74/`300], Step [40/46], Loss: 2.9850\n",
      "Epoch [75/`300], Step [10/46], Loss: 3.1526\n",
      "Epoch [75/`300], Step [20/46], Loss: 3.1426\n",
      "Epoch [75/`300], Step [30/46], Loss: 3.1111\n",
      "Epoch [75/`300], Step [40/46], Loss: 3.2983\n",
      "Epoch [76/`300], Step [10/46], Loss: 2.9578\n",
      "Epoch [76/`300], Step [20/46], Loss: 3.0530\n",
      "Epoch [76/`300], Step [30/46], Loss: 3.0521\n",
      "Epoch [76/`300], Step [40/46], Loss: 3.1462\n",
      "Epoch [77/`300], Step [10/46], Loss: 3.2452\n",
      "Epoch [77/`300], Step [20/46], Loss: 2.8776\n",
      "Epoch [77/`300], Step [30/46], Loss: 3.0829\n",
      "Epoch [77/`300], Step [40/46], Loss: 2.9420\n",
      "Epoch [78/`300], Step [10/46], Loss: 3.1321\n",
      "Epoch [78/`300], Step [20/46], Loss: 3.0745\n",
      "Epoch [78/`300], Step [30/46], Loss: 2.9877\n",
      "Epoch [78/`300], Step [40/46], Loss: 3.1867\n",
      "Epoch [79/`300], Step [10/46], Loss: 3.0300\n",
      "Epoch [79/`300], Step [20/46], Loss: 3.1290\n",
      "Epoch [79/`300], Step [30/46], Loss: 3.0695\n",
      "Epoch [79/`300], Step [40/46], Loss: 2.9253\n",
      "Epoch [80/`300], Step [10/46], Loss: 3.0676\n",
      "Epoch [80/`300], Step [20/46], Loss: 3.0767\n",
      "Epoch [80/`300], Step [30/46], Loss: 3.1184\n",
      "Epoch [80/`300], Step [40/46], Loss: 3.0931\n",
      "Epoch [81/`300], Step [10/46], Loss: 3.1118\n",
      "Epoch [81/`300], Step [20/46], Loss: 3.1709\n",
      "Epoch [81/`300], Step [30/46], Loss: 3.0030\n",
      "Epoch [81/`300], Step [40/46], Loss: 3.2389\n",
      "Epoch [82/`300], Step [10/46], Loss: 2.9305\n",
      "Epoch [82/`300], Step [20/46], Loss: 3.1084\n",
      "Epoch [82/`300], Step [30/46], Loss: 2.9614\n",
      "Epoch [82/`300], Step [40/46], Loss: 3.1462\n",
      "Epoch [83/`300], Step [10/46], Loss: 3.1421\n",
      "Epoch [83/`300], Step [20/46], Loss: 3.1978\n",
      "Epoch [83/`300], Step [30/46], Loss: 3.0319\n",
      "Epoch [83/`300], Step [40/46], Loss: 2.9588\n",
      "Epoch [84/`300], Step [10/46], Loss: 3.1178\n",
      "Epoch [84/`300], Step [20/46], Loss: 2.9509\n",
      "Epoch [84/`300], Step [30/46], Loss: 3.0517\n",
      "Epoch [84/`300], Step [40/46], Loss: 2.9734\n",
      "Epoch [85/`300], Step [10/46], Loss: 3.1196\n",
      "Epoch [85/`300], Step [20/46], Loss: 3.0155\n",
      "Epoch [85/`300], Step [30/46], Loss: 3.0492\n",
      "Epoch [85/`300], Step [40/46], Loss: 3.0697\n",
      "Epoch [86/`300], Step [10/46], Loss: 3.0790\n",
      "Epoch [86/`300], Step [20/46], Loss: 2.9491\n",
      "Epoch [86/`300], Step [30/46], Loss: 3.0417\n",
      "Epoch [86/`300], Step [40/46], Loss: 3.1030\n",
      "Epoch [87/`300], Step [10/46], Loss: 2.9861\n",
      "Epoch [87/`300], Step [20/46], Loss: 3.0970\n",
      "Epoch [87/`300], Step [30/46], Loss: 2.9945\n",
      "Epoch [87/`300], Step [40/46], Loss: 3.1065\n",
      "Epoch [88/`300], Step [10/46], Loss: 2.9161\n",
      "Epoch [88/`300], Step [20/46], Loss: 2.9322\n",
      "Epoch [88/`300], Step [30/46], Loss: 3.0617\n",
      "Epoch [88/`300], Step [40/46], Loss: 3.0251\n",
      "Epoch [89/`300], Step [10/46], Loss: 2.9068\n",
      "Epoch [89/`300], Step [20/46], Loss: 2.9539\n",
      "Epoch [89/`300], Step [30/46], Loss: 2.9946\n",
      "Epoch [89/`300], Step [40/46], Loss: 3.1537\n",
      "Epoch [90/`300], Step [10/46], Loss: 3.2553\n",
      "Epoch [90/`300], Step [20/46], Loss: 3.2086\n",
      "Epoch [90/`300], Step [30/46], Loss: 3.0702\n",
      "Epoch [90/`300], Step [40/46], Loss: 3.2638\n",
      "Epoch [91/`300], Step [10/46], Loss: 2.9955\n",
      "Epoch [91/`300], Step [20/46], Loss: 3.0603\n",
      "Epoch [91/`300], Step [30/46], Loss: 3.2227\n",
      "Epoch [91/`300], Step [40/46], Loss: 3.0036\n",
      "Epoch [92/`300], Step [10/46], Loss: 2.9818\n",
      "Epoch [92/`300], Step [20/46], Loss: 3.0402\n",
      "Epoch [92/`300], Step [30/46], Loss: 3.0237\n",
      "Epoch [92/`300], Step [40/46], Loss: 3.1345\n",
      "Epoch [93/`300], Step [10/46], Loss: 3.1455\n",
      "Epoch [93/`300], Step [20/46], Loss: 2.9691\n",
      "Epoch [93/`300], Step [30/46], Loss: 3.2126\n",
      "Epoch [93/`300], Step [40/46], Loss: 3.2490\n",
      "Epoch [94/`300], Step [10/46], Loss: 3.0561\n",
      "Epoch [94/`300], Step [20/46], Loss: 3.1309\n",
      "Epoch [94/`300], Step [30/46], Loss: 2.9278\n",
      "Epoch [94/`300], Step [40/46], Loss: 3.0176\n",
      "Epoch [95/`300], Step [10/46], Loss: 3.1054\n",
      "Epoch [95/`300], Step [20/46], Loss: 2.9630\n",
      "Epoch [95/`300], Step [30/46], Loss: 3.0315\n",
      "Epoch [95/`300], Step [40/46], Loss: 3.0352\n",
      "Epoch [96/`300], Step [10/46], Loss: 3.0147\n",
      "Epoch [96/`300], Step [20/46], Loss: 2.9542\n",
      "Epoch [96/`300], Step [30/46], Loss: 3.0232\n",
      "Epoch [96/`300], Step [40/46], Loss: 3.0569\n",
      "Epoch [97/`300], Step [10/46], Loss: 3.1267\n",
      "Epoch [97/`300], Step [20/46], Loss: 3.1341\n",
      "Epoch [97/`300], Step [30/46], Loss: 3.0839\n",
      "Epoch [97/`300], Step [40/46], Loss: 3.1084\n",
      "Epoch [98/`300], Step [10/46], Loss: 3.0530\n",
      "Epoch [98/`300], Step [20/46], Loss: 3.0731\n",
      "Epoch [98/`300], Step [30/46], Loss: 3.2156\n",
      "Epoch [98/`300], Step [40/46], Loss: 2.9959\n",
      "Epoch [99/`300], Step [10/46], Loss: 2.9669\n",
      "Epoch [99/`300], Step [20/46], Loss: 2.9601\n",
      "Epoch [99/`300], Step [30/46], Loss: 3.0323\n",
      "Epoch [99/`300], Step [40/46], Loss: 2.8316\n",
      "Epoch [100/`300], Step [10/46], Loss: 3.1232\n",
      "Epoch [100/`300], Step [20/46], Loss: 3.0016\n",
      "Epoch [100/`300], Step [30/46], Loss: 3.1266\n",
      "Epoch [100/`300], Step [40/46], Loss: 2.9669\n",
      "Epoch [101/`300], Step [10/46], Loss: 3.2500\n",
      "Epoch [101/`300], Step [20/46], Loss: 2.9196\n",
      "Epoch [101/`300], Step [30/46], Loss: 2.8368\n",
      "Epoch [101/`300], Step [40/46], Loss: 2.9943\n",
      "Epoch [102/`300], Step [10/46], Loss: 2.9602\n",
      "Epoch [102/`300], Step [20/46], Loss: 2.8989\n",
      "Epoch [102/`300], Step [30/46], Loss: 3.1402\n",
      "Epoch [102/`300], Step [40/46], Loss: 2.9899\n",
      "Epoch [103/`300], Step [10/46], Loss: 3.0839\n",
      "Epoch [103/`300], Step [20/46], Loss: 3.0054\n",
      "Epoch [103/`300], Step [30/46], Loss: 2.9814\n",
      "Epoch [103/`300], Step [40/46], Loss: 3.2039\n",
      "Epoch [104/`300], Step [10/46], Loss: 3.0559\n",
      "Epoch [104/`300], Step [20/46], Loss: 2.9302\n",
      "Epoch [104/`300], Step [30/46], Loss: 3.0349\n",
      "Epoch [104/`300], Step [40/46], Loss: 2.9571\n",
      "Epoch [105/`300], Step [10/46], Loss: 3.0774\n",
      "Epoch [105/`300], Step [20/46], Loss: 3.0069\n",
      "Epoch [105/`300], Step [30/46], Loss: 3.2799\n",
      "Epoch [105/`300], Step [40/46], Loss: 2.9380\n",
      "Epoch [106/`300], Step [10/46], Loss: 3.0865\n",
      "Epoch [106/`300], Step [20/46], Loss: 2.9310\n",
      "Epoch [106/`300], Step [30/46], Loss: 2.9493\n",
      "Epoch [106/`300], Step [40/46], Loss: 2.8556\n",
      "Epoch [107/`300], Step [10/46], Loss: 2.9220\n",
      "Epoch [107/`300], Step [20/46], Loss: 2.8835\n",
      "Epoch [107/`300], Step [30/46], Loss: 2.9409\n",
      "Epoch [107/`300], Step [40/46], Loss: 2.9611\n",
      "Epoch [108/`300], Step [10/46], Loss: 3.0931\n",
      "Epoch [108/`300], Step [20/46], Loss: 3.0987\n",
      "Epoch [108/`300], Step [30/46], Loss: 3.1360\n",
      "Epoch [108/`300], Step [40/46], Loss: 3.0474\n",
      "Epoch [109/`300], Step [10/46], Loss: 3.0632\n",
      "Epoch [109/`300], Step [20/46], Loss: 3.0778\n",
      "Epoch [109/`300], Step [30/46], Loss: 3.1368\n",
      "Epoch [109/`300], Step [40/46], Loss: 3.0818\n",
      "Epoch [110/`300], Step [10/46], Loss: 3.1061\n",
      "Epoch [110/`300], Step [20/46], Loss: 2.9980\n",
      "Epoch [110/`300], Step [30/46], Loss: 3.0753\n",
      "Epoch [110/`300], Step [40/46], Loss: 2.9566\n",
      "Epoch [111/`300], Step [10/46], Loss: 3.0365\n",
      "Epoch [111/`300], Step [20/46], Loss: 3.0214\n",
      "Epoch [111/`300], Step [30/46], Loss: 3.0593\n",
      "Epoch [111/`300], Step [40/46], Loss: 3.0393\n",
      "Epoch [112/`300], Step [10/46], Loss: 3.0084\n",
      "Epoch [112/`300], Step [20/46], Loss: 3.0008\n",
      "Epoch [112/`300], Step [30/46], Loss: 3.1418\n",
      "Epoch [112/`300], Step [40/46], Loss: 3.0137\n",
      "Epoch [113/`300], Step [10/46], Loss: 3.0450\n",
      "Epoch [113/`300], Step [20/46], Loss: 2.9544\n",
      "Epoch [113/`300], Step [30/46], Loss: 2.8558\n",
      "Epoch [113/`300], Step [40/46], Loss: 2.9676\n",
      "Epoch [114/`300], Step [10/46], Loss: 2.8938\n",
      "Epoch [114/`300], Step [20/46], Loss: 3.0149\n",
      "Epoch [114/`300], Step [30/46], Loss: 3.0836\n",
      "Epoch [114/`300], Step [40/46], Loss: 2.9660\n",
      "Epoch [115/`300], Step [10/46], Loss: 3.0709\n",
      "Epoch [115/`300], Step [20/46], Loss: 3.0626\n",
      "Epoch [115/`300], Step [30/46], Loss: 2.9609\n",
      "Epoch [115/`300], Step [40/46], Loss: 3.1219\n",
      "Epoch [116/`300], Step [10/46], Loss: 2.8943\n",
      "Epoch [116/`300], Step [20/46], Loss: 3.0432\n",
      "Epoch [116/`300], Step [30/46], Loss: 3.1315\n",
      "Epoch [116/`300], Step [40/46], Loss: 2.8289\n",
      "Epoch [117/`300], Step [10/46], Loss: 2.9796\n",
      "Epoch [117/`300], Step [20/46], Loss: 3.0931\n",
      "Epoch [117/`300], Step [30/46], Loss: 2.7433\n",
      "Epoch [117/`300], Step [40/46], Loss: 3.1512\n",
      "Epoch [118/`300], Step [10/46], Loss: 3.0289\n",
      "Epoch [118/`300], Step [20/46], Loss: 2.9598\n",
      "Epoch [118/`300], Step [30/46], Loss: 2.9099\n",
      "Epoch [118/`300], Step [40/46], Loss: 3.2115\n",
      "Epoch [119/`300], Step [10/46], Loss: 3.0257\n",
      "Epoch [119/`300], Step [20/46], Loss: 2.9194\n",
      "Epoch [119/`300], Step [30/46], Loss: 3.1436\n",
      "Epoch [119/`300], Step [40/46], Loss: 3.2120\n",
      "Epoch [120/`300], Step [10/46], Loss: 2.9017\n",
      "Epoch [120/`300], Step [20/46], Loss: 3.0429\n",
      "Epoch [120/`300], Step [30/46], Loss: 2.9082\n",
      "Epoch [120/`300], Step [40/46], Loss: 3.0821\n",
      "Epoch [121/`300], Step [10/46], Loss: 2.9675\n",
      "Epoch [121/`300], Step [20/46], Loss: 2.8457\n",
      "Epoch [121/`300], Step [30/46], Loss: 2.9806\n",
      "Epoch [121/`300], Step [40/46], Loss: 3.0073\n",
      "Epoch [122/`300], Step [10/46], Loss: 2.8423\n",
      "Epoch [122/`300], Step [20/46], Loss: 3.0497\n",
      "Epoch [122/`300], Step [30/46], Loss: 2.9611\n",
      "Epoch [122/`300], Step [40/46], Loss: 2.9503\n",
      "Epoch [123/`300], Step [10/46], Loss: 2.9933\n",
      "Epoch [123/`300], Step [20/46], Loss: 3.1052\n",
      "Epoch [123/`300], Step [30/46], Loss: 2.9268\n",
      "Epoch [123/`300], Step [40/46], Loss: 2.7557\n",
      "Epoch [124/`300], Step [10/46], Loss: 2.8906\n",
      "Epoch [124/`300], Step [20/46], Loss: 2.9728\n",
      "Epoch [124/`300], Step [30/46], Loss: 2.9542\n",
      "Epoch [124/`300], Step [40/46], Loss: 2.8622\n",
      "Epoch [125/`300], Step [10/46], Loss: 3.0583\n",
      "Epoch [125/`300], Step [20/46], Loss: 2.9768\n",
      "Epoch [125/`300], Step [30/46], Loss: 3.0007\n",
      "Epoch [125/`300], Step [40/46], Loss: 2.8608\n",
      "Epoch [126/`300], Step [10/46], Loss: 3.0707\n",
      "Epoch [126/`300], Step [20/46], Loss: 3.0676\n",
      "Epoch [126/`300], Step [30/46], Loss: 3.1140\n",
      "Epoch [126/`300], Step [40/46], Loss: 2.8569\n",
      "Epoch [127/`300], Step [10/46], Loss: 2.8924\n",
      "Epoch [127/`300], Step [20/46], Loss: 3.1411\n",
      "Epoch [127/`300], Step [30/46], Loss: 3.0077\n",
      "Epoch [127/`300], Step [40/46], Loss: 3.1088\n",
      "Epoch [128/`300], Step [10/46], Loss: 2.9381\n",
      "Epoch [128/`300], Step [20/46], Loss: 3.1016\n",
      "Epoch [128/`300], Step [30/46], Loss: 3.0170\n",
      "Epoch [128/`300], Step [40/46], Loss: 2.8899\n",
      "Epoch [129/`300], Step [10/46], Loss: 3.0706\n",
      "Epoch [129/`300], Step [20/46], Loss: 3.0503\n",
      "Epoch [129/`300], Step [30/46], Loss: 3.0927\n",
      "Epoch [129/`300], Step [40/46], Loss: 2.8530\n",
      "Epoch [130/`300], Step [10/46], Loss: 2.9250\n",
      "Epoch [130/`300], Step [20/46], Loss: 2.9062\n",
      "Epoch [130/`300], Step [30/46], Loss: 3.0386\n",
      "Epoch [130/`300], Step [40/46], Loss: 2.8332\n",
      "Epoch [131/`300], Step [10/46], Loss: 2.8908\n",
      "Epoch [131/`300], Step [20/46], Loss: 2.9679\n",
      "Epoch [131/`300], Step [30/46], Loss: 3.0730\n",
      "Epoch [131/`300], Step [40/46], Loss: 3.1035\n",
      "Epoch [132/`300], Step [10/46], Loss: 3.0689\n",
      "Epoch [132/`300], Step [20/46], Loss: 2.9914\n",
      "Epoch [132/`300], Step [30/46], Loss: 2.9925\n",
      "Epoch [132/`300], Step [40/46], Loss: 3.0165\n",
      "Epoch [133/`300], Step [10/46], Loss: 2.9580\n",
      "Epoch [133/`300], Step [20/46], Loss: 2.9168\n",
      "Epoch [133/`300], Step [30/46], Loss: 2.9518\n",
      "Epoch [133/`300], Step [40/46], Loss: 2.9842\n",
      "Epoch [134/`300], Step [10/46], Loss: 3.0050\n",
      "Epoch [134/`300], Step [20/46], Loss: 2.9642\n",
      "Epoch [134/`300], Step [30/46], Loss: 2.9258\n",
      "Epoch [134/`300], Step [40/46], Loss: 2.8446\n",
      "Epoch [135/`300], Step [10/46], Loss: 2.9340\n",
      "Epoch [135/`300], Step [20/46], Loss: 3.0343\n",
      "Epoch [135/`300], Step [30/46], Loss: 2.8771\n",
      "Epoch [135/`300], Step [40/46], Loss: 3.1298\n",
      "Epoch [136/`300], Step [10/46], Loss: 2.9627\n",
      "Epoch [136/`300], Step [20/46], Loss: 2.9398\n",
      "Epoch [136/`300], Step [30/46], Loss: 3.0075\n",
      "Epoch [136/`300], Step [40/46], Loss: 3.0645\n",
      "Epoch [137/`300], Step [10/46], Loss: 2.9034\n",
      "Epoch [137/`300], Step [20/46], Loss: 3.1691\n",
      "Epoch [137/`300], Step [30/46], Loss: 2.9006\n",
      "Epoch [137/`300], Step [40/46], Loss: 3.0552\n",
      "Epoch [138/`300], Step [10/46], Loss: 3.0433\n",
      "Epoch [138/`300], Step [20/46], Loss: 2.9363\n",
      "Epoch [138/`300], Step [30/46], Loss: 2.9796\n",
      "Epoch [138/`300], Step [40/46], Loss: 2.8942\n",
      "Epoch [139/`300], Step [10/46], Loss: 3.0263\n",
      "Epoch [139/`300], Step [20/46], Loss: 3.0211\n",
      "Epoch [139/`300], Step [30/46], Loss: 3.0087\n",
      "Epoch [139/`300], Step [40/46], Loss: 3.0318\n",
      "Epoch [140/`300], Step [10/46], Loss: 2.8637\n",
      "Epoch [140/`300], Step [20/46], Loss: 2.8738\n",
      "Epoch [140/`300], Step [30/46], Loss: 2.8689\n",
      "Epoch [140/`300], Step [40/46], Loss: 3.2249\n",
      "Epoch [141/`300], Step [10/46], Loss: 3.1549\n",
      "Epoch [141/`300], Step [20/46], Loss: 2.9251\n",
      "Epoch [141/`300], Step [30/46], Loss: 3.0037\n",
      "Epoch [141/`300], Step [40/46], Loss: 3.0660\n",
      "Epoch [142/`300], Step [10/46], Loss: 3.1434\n",
      "Epoch [142/`300], Step [20/46], Loss: 2.9235\n",
      "Epoch [142/`300], Step [30/46], Loss: 2.9024\n",
      "Epoch [142/`300], Step [40/46], Loss: 3.1797\n",
      "Epoch [143/`300], Step [10/46], Loss: 3.0319\n",
      "Epoch [143/`300], Step [20/46], Loss: 2.8418\n",
      "Epoch [143/`300], Step [30/46], Loss: 2.9151\n",
      "Epoch [143/`300], Step [40/46], Loss: 2.9051\n",
      "Epoch [144/`300], Step [10/46], Loss: 2.9675\n",
      "Epoch [144/`300], Step [20/46], Loss: 3.1934\n",
      "Epoch [144/`300], Step [30/46], Loss: 2.9576\n",
      "Epoch [144/`300], Step [40/46], Loss: 2.8826\n",
      "Epoch [145/`300], Step [10/46], Loss: 2.8814\n",
      "Epoch [145/`300], Step [20/46], Loss: 2.9850\n",
      "Epoch [145/`300], Step [30/46], Loss: 2.9156\n",
      "Epoch [145/`300], Step [40/46], Loss: 3.0052\n",
      "Epoch [146/`300], Step [10/46], Loss: 3.0321\n",
      "Epoch [146/`300], Step [20/46], Loss: 2.9283\n",
      "Epoch [146/`300], Step [30/46], Loss: 2.9047\n",
      "Epoch [146/`300], Step [40/46], Loss: 3.0326\n",
      "Epoch [147/`300], Step [10/46], Loss: 3.0728\n",
      "Epoch [147/`300], Step [20/46], Loss: 2.9324\n",
      "Epoch [147/`300], Step [30/46], Loss: 2.9682\n",
      "Epoch [147/`300], Step [40/46], Loss: 3.0629\n",
      "Epoch [148/`300], Step [10/46], Loss: 3.0222\n",
      "Epoch [148/`300], Step [20/46], Loss: 3.0051\n",
      "Epoch [148/`300], Step [30/46], Loss: 2.8520\n",
      "Epoch [148/`300], Step [40/46], Loss: 3.0408\n",
      "Epoch [149/`300], Step [10/46], Loss: 3.1116\n",
      "Epoch [149/`300], Step [20/46], Loss: 3.1063\n",
      "Epoch [149/`300], Step [30/46], Loss: 2.9411\n",
      "Epoch [149/`300], Step [40/46], Loss: 2.8491\n",
      "Epoch [150/`300], Step [10/46], Loss: 2.9711\n",
      "Epoch [150/`300], Step [20/46], Loss: 2.9807\n",
      "Epoch [150/`300], Step [30/46], Loss: 2.8511\n",
      "Epoch [150/`300], Step [40/46], Loss: 2.9812\n",
      "Epoch [151/`300], Step [10/46], Loss: 2.9297\n",
      "Epoch [151/`300], Step [20/46], Loss: 2.8950\n",
      "Epoch [151/`300], Step [30/46], Loss: 3.0208\n",
      "Epoch [151/`300], Step [40/46], Loss: 3.0943\n",
      "Epoch [152/`300], Step [10/46], Loss: 2.9601\n",
      "Epoch [152/`300], Step [20/46], Loss: 3.0128\n",
      "Epoch [152/`300], Step [30/46], Loss: 2.9658\n",
      "Epoch [152/`300], Step [40/46], Loss: 2.9356\n",
      "Epoch [153/`300], Step [10/46], Loss: 2.9412\n",
      "Epoch [153/`300], Step [20/46], Loss: 2.7680\n",
      "Epoch [153/`300], Step [30/46], Loss: 3.1228\n",
      "Epoch [153/`300], Step [40/46], Loss: 3.0782\n",
      "Epoch [154/`300], Step [10/46], Loss: 2.8683\n",
      "Epoch [154/`300], Step [20/46], Loss: 3.0346\n",
      "Epoch [154/`300], Step [30/46], Loss: 2.9921\n",
      "Epoch [154/`300], Step [40/46], Loss: 2.9070\n",
      "Epoch [155/`300], Step [10/46], Loss: 2.9474\n",
      "Epoch [155/`300], Step [20/46], Loss: 3.0534\n",
      "Epoch [155/`300], Step [30/46], Loss: 3.0585\n",
      "Epoch [155/`300], Step [40/46], Loss: 2.9324\n",
      "Epoch [156/`300], Step [10/46], Loss: 2.8866\n",
      "Epoch [156/`300], Step [20/46], Loss: 2.8314\n",
      "Epoch [156/`300], Step [30/46], Loss: 3.0013\n",
      "Epoch [156/`300], Step [40/46], Loss: 3.0655\n",
      "Epoch [157/`300], Step [10/46], Loss: 2.8519\n",
      "Epoch [157/`300], Step [20/46], Loss: 2.9493\n",
      "Epoch [157/`300], Step [30/46], Loss: 3.0132\n",
      "Epoch [157/`300], Step [40/46], Loss: 2.9632\n",
      "Epoch [158/`300], Step [10/46], Loss: 2.9613\n",
      "Epoch [158/`300], Step [20/46], Loss: 2.9506\n",
      "Epoch [158/`300], Step [30/46], Loss: 2.9528\n",
      "Epoch [158/`300], Step [40/46], Loss: 3.0204\n",
      "Epoch [159/`300], Step [10/46], Loss: 2.9617\n",
      "Epoch [159/`300], Step [20/46], Loss: 2.9668\n",
      "Epoch [159/`300], Step [30/46], Loss: 3.0115\n",
      "Epoch [159/`300], Step [40/46], Loss: 2.8424\n",
      "Epoch [160/`300], Step [10/46], Loss: 2.8851\n",
      "Epoch [160/`300], Step [20/46], Loss: 3.0281\n",
      "Epoch [160/`300], Step [30/46], Loss: 2.7497\n",
      "Epoch [160/`300], Step [40/46], Loss: 3.0236\n",
      "Epoch [161/`300], Step [10/46], Loss: 2.9139\n",
      "Epoch [161/`300], Step [20/46], Loss: 3.1467\n",
      "Epoch [161/`300], Step [30/46], Loss: 3.1464\n",
      "Epoch [161/`300], Step [40/46], Loss: 3.1053\n",
      "Epoch [162/`300], Step [10/46], Loss: 2.8839\n",
      "Epoch [162/`300], Step [20/46], Loss: 2.8659\n",
      "Epoch [162/`300], Step [30/46], Loss: 3.1049\n",
      "Epoch [162/`300], Step [40/46], Loss: 2.9468\n",
      "Epoch [163/`300], Step [10/46], Loss: 2.9146\n",
      "Epoch [163/`300], Step [20/46], Loss: 2.7512\n",
      "Epoch [163/`300], Step [30/46], Loss: 2.9948\n",
      "Epoch [163/`300], Step [40/46], Loss: 3.2408\n",
      "Epoch [164/`300], Step [10/46], Loss: 3.0820\n",
      "Epoch [164/`300], Step [20/46], Loss: 2.9667\n",
      "Epoch [164/`300], Step [30/46], Loss: 2.9074\n",
      "Epoch [164/`300], Step [40/46], Loss: 3.0323\n",
      "Epoch [165/`300], Step [10/46], Loss: 2.8667\n",
      "Epoch [165/`300], Step [20/46], Loss: 3.0450\n",
      "Epoch [165/`300], Step [30/46], Loss: 2.9220\n",
      "Epoch [165/`300], Step [40/46], Loss: 3.0452\n",
      "Epoch [166/`300], Step [10/46], Loss: 2.9483\n",
      "Epoch [166/`300], Step [20/46], Loss: 2.8849\n",
      "Epoch [166/`300], Step [30/46], Loss: 2.8392\n",
      "Epoch [166/`300], Step [40/46], Loss: 2.8616\n",
      "Epoch [167/`300], Step [10/46], Loss: 2.9533\n",
      "Epoch [167/`300], Step [20/46], Loss: 3.0001\n",
      "Epoch [167/`300], Step [30/46], Loss: 2.9168\n",
      "Epoch [167/`300], Step [40/46], Loss: 2.9646\n",
      "Epoch [168/`300], Step [10/46], Loss: 2.8620\n",
      "Epoch [168/`300], Step [20/46], Loss: 2.8706\n",
      "Epoch [168/`300], Step [30/46], Loss: 3.0288\n",
      "Epoch [168/`300], Step [40/46], Loss: 3.0626\n",
      "Epoch [169/`300], Step [10/46], Loss: 2.9963\n",
      "Epoch [169/`300], Step [20/46], Loss: 2.8559\n",
      "Epoch [169/`300], Step [30/46], Loss: 2.9458\n",
      "Epoch [169/`300], Step [40/46], Loss: 2.9218\n",
      "Epoch [170/`300], Step [10/46], Loss: 3.0168\n",
      "Epoch [170/`300], Step [20/46], Loss: 3.0295\n",
      "Epoch [170/`300], Step [30/46], Loss: 3.0581\n",
      "Epoch [170/`300], Step [40/46], Loss: 3.0238\n",
      "Epoch [171/`300], Step [10/46], Loss: 2.9604\n",
      "Epoch [171/`300], Step [20/46], Loss: 2.8812\n",
      "Epoch [171/`300], Step [30/46], Loss: 3.0097\n",
      "Epoch [171/`300], Step [40/46], Loss: 2.8524\n",
      "Epoch [172/`300], Step [10/46], Loss: 3.0641\n",
      "Epoch [172/`300], Step [20/46], Loss: 2.8862\n",
      "Epoch [172/`300], Step [30/46], Loss: 2.9143\n",
      "Epoch [172/`300], Step [40/46], Loss: 3.1281\n",
      "Epoch [173/`300], Step [10/46], Loss: 2.9510\n",
      "Epoch [173/`300], Step [20/46], Loss: 2.9907\n",
      "Epoch [173/`300], Step [30/46], Loss: 2.9561\n",
      "Epoch [173/`300], Step [40/46], Loss: 2.9572\n",
      "Epoch [174/`300], Step [10/46], Loss: 2.8887\n",
      "Epoch [174/`300], Step [20/46], Loss: 2.7719\n",
      "Epoch [174/`300], Step [30/46], Loss: 3.0435\n",
      "Epoch [174/`300], Step [40/46], Loss: 3.0141\n",
      "Epoch [175/`300], Step [10/46], Loss: 2.9525\n",
      "Epoch [175/`300], Step [20/46], Loss: 2.9454\n",
      "Epoch [175/`300], Step [30/46], Loss: 2.8088\n",
      "Epoch [175/`300], Step [40/46], Loss: 2.9736\n",
      "Epoch [176/`300], Step [10/46], Loss: 2.9656\n",
      "Epoch [176/`300], Step [20/46], Loss: 3.0713\n",
      "Epoch [176/`300], Step [30/46], Loss: 3.0407\n",
      "Epoch [176/`300], Step [40/46], Loss: 2.9001\n",
      "Epoch [177/`300], Step [10/46], Loss: 2.7801\n",
      "Epoch [177/`300], Step [20/46], Loss: 2.8496\n",
      "Epoch [177/`300], Step [30/46], Loss: 3.1156\n",
      "Epoch [177/`300], Step [40/46], Loss: 2.9436\n",
      "Epoch [178/`300], Step [10/46], Loss: 2.8825\n",
      "Epoch [178/`300], Step [20/46], Loss: 2.8327\n",
      "Epoch [178/`300], Step [30/46], Loss: 2.8696\n",
      "Epoch [178/`300], Step [40/46], Loss: 2.8952\n",
      "Epoch [179/`300], Step [10/46], Loss: 2.9950\n",
      "Epoch [179/`300], Step [20/46], Loss: 2.6827\n",
      "Epoch [179/`300], Step [30/46], Loss: 2.9267\n",
      "Epoch [179/`300], Step [40/46], Loss: 3.0426\n",
      "Epoch [180/`300], Step [10/46], Loss: 2.9106\n",
      "Epoch [180/`300], Step [20/46], Loss: 2.9356\n",
      "Epoch [180/`300], Step [30/46], Loss: 2.9388\n",
      "Epoch [180/`300], Step [40/46], Loss: 2.8423\n",
      "Epoch [181/`300], Step [10/46], Loss: 2.7737\n",
      "Epoch [181/`300], Step [20/46], Loss: 2.9312\n",
      "Epoch [181/`300], Step [30/46], Loss: 2.8892\n",
      "Epoch [181/`300], Step [40/46], Loss: 3.0807\n",
      "Epoch [182/`300], Step [10/46], Loss: 2.9356\n",
      "Epoch [182/`300], Step [20/46], Loss: 3.1249\n",
      "Epoch [182/`300], Step [30/46], Loss: 2.9241\n",
      "Epoch [182/`300], Step [40/46], Loss: 2.9552\n",
      "Epoch [183/`300], Step [10/46], Loss: 2.8467\n",
      "Epoch [183/`300], Step [20/46], Loss: 2.9216\n",
      "Epoch [183/`300], Step [30/46], Loss: 3.1525\n",
      "Epoch [183/`300], Step [40/46], Loss: 2.9235\n",
      "Epoch [184/`300], Step [10/46], Loss: 3.0374\n",
      "Epoch [184/`300], Step [20/46], Loss: 2.9552\n",
      "Epoch [184/`300], Step [30/46], Loss: 3.0587\n",
      "Epoch [184/`300], Step [40/46], Loss: 3.0313\n",
      "Epoch [185/`300], Step [10/46], Loss: 2.9538\n",
      "Epoch [185/`300], Step [20/46], Loss: 2.9391\n",
      "Epoch [185/`300], Step [30/46], Loss: 3.0776\n",
      "Epoch [185/`300], Step [40/46], Loss: 2.9838\n",
      "Epoch [186/`300], Step [10/46], Loss: 3.0517\n",
      "Epoch [186/`300], Step [20/46], Loss: 2.8133\n",
      "Epoch [186/`300], Step [30/46], Loss: 2.9199\n",
      "Epoch [186/`300], Step [40/46], Loss: 2.8424\n",
      "Epoch [187/`300], Step [10/46], Loss: 2.9494\n",
      "Epoch [187/`300], Step [20/46], Loss: 3.0055\n",
      "Epoch [187/`300], Step [30/46], Loss: 2.8401\n",
      "Epoch [187/`300], Step [40/46], Loss: 2.8593\n",
      "Epoch [188/`300], Step [10/46], Loss: 2.9159\n",
      "Epoch [188/`300], Step [20/46], Loss: 3.1303\n",
      "Epoch [188/`300], Step [30/46], Loss: 2.9729\n",
      "Epoch [188/`300], Step [40/46], Loss: 2.8408\n",
      "Epoch [189/`300], Step [10/46], Loss: 2.9285\n",
      "Epoch [189/`300], Step [20/46], Loss: 2.9553\n",
      "Epoch [189/`300], Step [30/46], Loss: 2.9502\n",
      "Epoch [189/`300], Step [40/46], Loss: 2.8870\n",
      "Epoch [190/`300], Step [10/46], Loss: 2.7088\n",
      "Epoch [190/`300], Step [20/46], Loss: 2.8907\n",
      "Epoch [190/`300], Step [30/46], Loss: 2.9610\n",
      "Epoch [190/`300], Step [40/46], Loss: 2.9041\n",
      "Epoch [191/`300], Step [10/46], Loss: 3.0842\n",
      "Epoch [191/`300], Step [20/46], Loss: 2.8533\n",
      "Epoch [191/`300], Step [30/46], Loss: 2.9640\n",
      "Epoch [191/`300], Step [40/46], Loss: 2.9561\n",
      "Epoch [192/`300], Step [10/46], Loss: 2.8038\n",
      "Epoch [192/`300], Step [20/46], Loss: 2.7532\n",
      "Epoch [192/`300], Step [30/46], Loss: 3.0530\n",
      "Epoch [192/`300], Step [40/46], Loss: 2.9202\n",
      "Epoch [193/`300], Step [10/46], Loss: 2.7848\n",
      "Epoch [193/`300], Step [20/46], Loss: 2.7568\n",
      "Epoch [193/`300], Step [30/46], Loss: 3.0527\n",
      "Epoch [193/`300], Step [40/46], Loss: 2.9078\n",
      "Epoch [194/`300], Step [10/46], Loss: 2.8832\n",
      "Epoch [194/`300], Step [20/46], Loss: 3.1439\n",
      "Epoch [194/`300], Step [30/46], Loss: 3.0855\n",
      "Epoch [194/`300], Step [40/46], Loss: 2.8267\n",
      "Epoch [195/`300], Step [10/46], Loss: 2.9430\n",
      "Epoch [195/`300], Step [20/46], Loss: 2.9265\n",
      "Epoch [195/`300], Step [30/46], Loss: 2.9585\n",
      "Epoch [195/`300], Step [40/46], Loss: 3.0014\n",
      "Epoch [196/`300], Step [10/46], Loss: 2.9419\n",
      "Epoch [196/`300], Step [20/46], Loss: 2.9493\n",
      "Epoch [196/`300], Step [30/46], Loss: 3.0042\n",
      "Epoch [196/`300], Step [40/46], Loss: 3.0006\n",
      "Epoch [197/`300], Step [10/46], Loss: 2.9364\n",
      "Epoch [197/`300], Step [20/46], Loss: 2.8097\n",
      "Epoch [197/`300], Step [30/46], Loss: 2.9795\n",
      "Epoch [197/`300], Step [40/46], Loss: 2.8795\n",
      "Epoch [198/`300], Step [10/46], Loss: 2.8235\n",
      "Epoch [198/`300], Step [20/46], Loss: 2.9612\n",
      "Epoch [198/`300], Step [30/46], Loss: 2.9933\n",
      "Epoch [198/`300], Step [40/46], Loss: 2.8262\n",
      "Epoch [199/`300], Step [10/46], Loss: 2.8136\n",
      "Epoch [199/`300], Step [20/46], Loss: 3.0160\n",
      "Epoch [199/`300], Step [30/46], Loss: 2.9526\n",
      "Epoch [199/`300], Step [40/46], Loss: 2.7976\n",
      "Epoch [200/`300], Step [10/46], Loss: 2.9874\n",
      "Epoch [200/`300], Step [20/46], Loss: 2.9670\n",
      "Epoch [200/`300], Step [30/46], Loss: 2.9223\n",
      "Epoch [200/`300], Step [40/46], Loss: 2.8547\n",
      "Epoch [201/`300], Step [10/46], Loss: 2.9776\n",
      "Epoch [201/`300], Step [20/46], Loss: 2.9100\n",
      "Epoch [201/`300], Step [30/46], Loss: 2.9146\n",
      "Epoch [201/`300], Step [40/46], Loss: 3.0137\n",
      "Epoch [202/`300], Step [10/46], Loss: 2.9852\n",
      "Epoch [202/`300], Step [20/46], Loss: 2.9505\n",
      "Epoch [202/`300], Step [30/46], Loss: 3.0038\n",
      "Epoch [202/`300], Step [40/46], Loss: 2.8272\n",
      "Epoch [203/`300], Step [10/46], Loss: 2.8669\n",
      "Epoch [203/`300], Step [20/46], Loss: 2.9021\n",
      "Epoch [203/`300], Step [30/46], Loss: 2.9167\n",
      "Epoch [203/`300], Step [40/46], Loss: 2.8527\n",
      "Epoch [204/`300], Step [10/46], Loss: 2.9596\n",
      "Epoch [204/`300], Step [20/46], Loss: 3.0212\n",
      "Epoch [204/`300], Step [30/46], Loss: 2.9466\n",
      "Epoch [204/`300], Step [40/46], Loss: 2.8568\n",
      "Epoch [205/`300], Step [10/46], Loss: 2.9004\n",
      "Epoch [205/`300], Step [20/46], Loss: 2.9274\n",
      "Epoch [205/`300], Step [30/46], Loss: 2.8854\n",
      "Epoch [205/`300], Step [40/46], Loss: 3.0721\n",
      "Epoch [206/`300], Step [10/46], Loss: 2.9582\n",
      "Epoch [206/`300], Step [20/46], Loss: 2.7396\n",
      "Epoch [206/`300], Step [30/46], Loss: 2.9116\n",
      "Epoch [206/`300], Step [40/46], Loss: 2.8362\n",
      "Epoch [207/`300], Step [10/46], Loss: 2.8546\n",
      "Epoch [207/`300], Step [20/46], Loss: 2.9578\n",
      "Epoch [207/`300], Step [30/46], Loss: 2.9197\n",
      "Epoch [207/`300], Step [40/46], Loss: 2.9430\n",
      "Epoch [208/`300], Step [10/46], Loss: 2.9268\n",
      "Epoch [208/`300], Step [20/46], Loss: 2.9484\n",
      "Epoch [208/`300], Step [30/46], Loss: 2.8395\n",
      "Epoch [208/`300], Step [40/46], Loss: 3.0086\n",
      "Epoch [209/`300], Step [10/46], Loss: 2.8618\n",
      "Epoch [209/`300], Step [20/46], Loss: 2.9450\n",
      "Epoch [209/`300], Step [30/46], Loss: 2.9847\n",
      "Epoch [209/`300], Step [40/46], Loss: 2.7813\n",
      "Epoch [210/`300], Step [10/46], Loss: 2.7922\n",
      "Epoch [210/`300], Step [20/46], Loss: 2.9670\n",
      "Epoch [210/`300], Step [30/46], Loss: 2.9013\n",
      "Epoch [210/`300], Step [40/46], Loss: 3.1242\n",
      "Epoch [211/`300], Step [10/46], Loss: 3.0038\n",
      "Epoch [211/`300], Step [20/46], Loss: 2.9006\n",
      "Epoch [211/`300], Step [30/46], Loss: 2.8499\n",
      "Epoch [211/`300], Step [40/46], Loss: 2.8850\n",
      "Epoch [212/`300], Step [10/46], Loss: 2.8552\n",
      "Epoch [212/`300], Step [20/46], Loss: 2.9202\n",
      "Epoch [212/`300], Step [30/46], Loss: 2.9577\n",
      "Epoch [212/`300], Step [40/46], Loss: 2.9065\n",
      "Epoch [213/`300], Step [10/46], Loss: 2.9091\n",
      "Epoch [213/`300], Step [20/46], Loss: 2.8549\n",
      "Epoch [213/`300], Step [30/46], Loss: 2.8225\n",
      "Epoch [213/`300], Step [40/46], Loss: 3.0304\n",
      "Epoch [214/`300], Step [10/46], Loss: 2.8730\n",
      "Epoch [214/`300], Step [20/46], Loss: 2.7656\n",
      "Epoch [214/`300], Step [30/46], Loss: 2.8293\n",
      "Epoch [214/`300], Step [40/46], Loss: 2.8829\n",
      "Epoch [215/`300], Step [10/46], Loss: 2.9393\n",
      "Epoch [215/`300], Step [20/46], Loss: 2.9180\n",
      "Epoch [215/`300], Step [30/46], Loss: 2.8769\n",
      "Epoch [215/`300], Step [40/46], Loss: 2.8032\n",
      "Epoch [216/`300], Step [10/46], Loss: 3.1021\n",
      "Epoch [216/`300], Step [20/46], Loss: 2.8392\n",
      "Epoch [216/`300], Step [30/46], Loss: 3.0478\n",
      "Epoch [216/`300], Step [40/46], Loss: 2.7634\n",
      "Epoch [217/`300], Step [10/46], Loss: 2.9131\n",
      "Epoch [217/`300], Step [20/46], Loss: 2.8364\n",
      "Epoch [217/`300], Step [30/46], Loss: 2.8643\n",
      "Epoch [217/`300], Step [40/46], Loss: 2.8497\n",
      "Epoch [218/`300], Step [10/46], Loss: 2.7441\n",
      "Epoch [218/`300], Step [20/46], Loss: 2.9058\n",
      "Epoch [218/`300], Step [30/46], Loss: 2.8578\n",
      "Epoch [218/`300], Step [40/46], Loss: 2.9081\n",
      "Epoch [219/`300], Step [10/46], Loss: 2.9393\n",
      "Epoch [219/`300], Step [20/46], Loss: 3.0095\n",
      "Epoch [219/`300], Step [30/46], Loss: 2.8499\n",
      "Epoch [219/`300], Step [40/46], Loss: 2.9539\n",
      "Epoch [220/`300], Step [10/46], Loss: 2.7775\n",
      "Epoch [220/`300], Step [20/46], Loss: 2.7650\n",
      "Epoch [220/`300], Step [30/46], Loss: 3.0141\n",
      "Epoch [220/`300], Step [40/46], Loss: 3.0073\n",
      "Epoch [221/`300], Step [10/46], Loss: 2.8737\n",
      "Epoch [221/`300], Step [20/46], Loss: 2.9907\n",
      "Epoch [221/`300], Step [30/46], Loss: 2.7572\n",
      "Epoch [221/`300], Step [40/46], Loss: 2.9129\n",
      "Epoch [222/`300], Step [10/46], Loss: 2.8782\n",
      "Epoch [222/`300], Step [20/46], Loss: 2.8299\n",
      "Epoch [222/`300], Step [30/46], Loss: 2.8937\n",
      "Epoch [222/`300], Step [40/46], Loss: 2.8659\n",
      "Epoch [223/`300], Step [10/46], Loss: 2.8390\n",
      "Epoch [223/`300], Step [20/46], Loss: 3.0418\n",
      "Epoch [223/`300], Step [30/46], Loss: 3.0938\n",
      "Epoch [223/`300], Step [40/46], Loss: 2.8465\n",
      "Epoch [224/`300], Step [10/46], Loss: 2.9029\n",
      "Epoch [224/`300], Step [20/46], Loss: 2.8699\n",
      "Epoch [224/`300], Step [30/46], Loss: 2.9346\n",
      "Epoch [224/`300], Step [40/46], Loss: 2.7859\n",
      "Epoch [225/`300], Step [10/46], Loss: 2.8813\n",
      "Epoch [225/`300], Step [20/46], Loss: 2.7476\n",
      "Epoch [225/`300], Step [30/46], Loss: 2.8606\n",
      "Epoch [225/`300], Step [40/46], Loss: 3.0311\n",
      "Epoch [226/`300], Step [10/46], Loss: 2.8805\n",
      "Epoch [226/`300], Step [20/46], Loss: 2.9512\n",
      "Epoch [226/`300], Step [30/46], Loss: 2.9677\n",
      "Epoch [226/`300], Step [40/46], Loss: 2.9688\n",
      "Epoch [227/`300], Step [10/46], Loss: 2.7229\n",
      "Epoch [227/`300], Step [20/46], Loss: 2.8058\n",
      "Epoch [227/`300], Step [30/46], Loss: 3.0159\n",
      "Epoch [227/`300], Step [40/46], Loss: 2.9460\n",
      "Epoch [228/`300], Step [10/46], Loss: 2.7642\n",
      "Epoch [228/`300], Step [20/46], Loss: 2.9087\n",
      "Epoch [228/`300], Step [30/46], Loss: 2.9397\n",
      "Epoch [228/`300], Step [40/46], Loss: 2.9024\n",
      "Epoch [229/`300], Step [10/46], Loss: 2.8310\n",
      "Epoch [229/`300], Step [20/46], Loss: 2.8688\n",
      "Epoch [229/`300], Step [30/46], Loss: 2.8547\n",
      "Epoch [229/`300], Step [40/46], Loss: 2.9622\n",
      "Epoch [230/`300], Step [10/46], Loss: 2.7735\n",
      "Epoch [230/`300], Step [20/46], Loss: 2.9087\n",
      "Epoch [230/`300], Step [30/46], Loss: 2.9345\n",
      "Epoch [230/`300], Step [40/46], Loss: 2.9723\n",
      "Epoch [231/`300], Step [10/46], Loss: 2.9168\n",
      "Epoch [231/`300], Step [20/46], Loss: 2.9442\n",
      "Epoch [231/`300], Step [30/46], Loss: 2.8587\n",
      "Epoch [231/`300], Step [40/46], Loss: 2.8834\n",
      "Epoch [232/`300], Step [10/46], Loss: 2.9902\n",
      "Epoch [232/`300], Step [20/46], Loss: 2.8559\n",
      "Epoch [232/`300], Step [30/46], Loss: 2.9499\n",
      "Epoch [232/`300], Step [40/46], Loss: 2.8569\n",
      "Epoch [233/`300], Step [10/46], Loss: 2.8466\n",
      "Epoch [233/`300], Step [20/46], Loss: 2.8656\n",
      "Epoch [233/`300], Step [30/46], Loss: 3.0169\n",
      "Epoch [233/`300], Step [40/46], Loss: 2.7842\n",
      "Epoch [234/`300], Step [10/46], Loss: 2.7627\n",
      "Epoch [234/`300], Step [20/46], Loss: 2.9973\n",
      "Epoch [234/`300], Step [30/46], Loss: 2.8157\n",
      "Epoch [234/`300], Step [40/46], Loss: 2.8337\n",
      "Epoch [235/`300], Step [10/46], Loss: 2.9711\n",
      "Epoch [235/`300], Step [20/46], Loss: 2.9569\n",
      "Epoch [235/`300], Step [30/46], Loss: 2.7786\n",
      "Epoch [235/`300], Step [40/46], Loss: 2.8992\n",
      "Epoch [236/`300], Step [10/46], Loss: 2.8970\n",
      "Epoch [236/`300], Step [20/46], Loss: 2.8576\n",
      "Epoch [236/`300], Step [30/46], Loss: 2.6934\n",
      "Epoch [236/`300], Step [40/46], Loss: 2.9669\n",
      "Epoch [237/`300], Step [10/46], Loss: 2.8176\n",
      "Epoch [237/`300], Step [20/46], Loss: 3.0389\n",
      "Epoch [237/`300], Step [30/46], Loss: 2.9427\n",
      "Epoch [237/`300], Step [40/46], Loss: 2.8519\n",
      "Epoch [238/`300], Step [10/46], Loss: 2.7974\n",
      "Epoch [238/`300], Step [20/46], Loss: 2.8441\n",
      "Epoch [238/`300], Step [30/46], Loss: 2.9024\n",
      "Epoch [238/`300], Step [40/46], Loss: 2.8633\n",
      "Epoch [239/`300], Step [10/46], Loss: 2.7283\n",
      "Epoch [239/`300], Step [20/46], Loss: 2.9762\n",
      "Epoch [239/`300], Step [30/46], Loss: 2.9350\n",
      "Epoch [239/`300], Step [40/46], Loss: 3.0525\n",
      "Epoch [240/`300], Step [10/46], Loss: 2.8191\n",
      "Epoch [240/`300], Step [20/46], Loss: 2.8752\n",
      "Epoch [240/`300], Step [30/46], Loss: 2.7466\n",
      "Epoch [240/`300], Step [40/46], Loss: 2.7858\n",
      "Epoch [241/`300], Step [10/46], Loss: 2.8297\n",
      "Epoch [241/`300], Step [20/46], Loss: 2.8938\n",
      "Epoch [241/`300], Step [30/46], Loss: 3.0368\n",
      "Epoch [241/`300], Step [40/46], Loss: 2.9640\n",
      "Epoch [242/`300], Step [10/46], Loss: 2.8799\n",
      "Epoch [242/`300], Step [20/46], Loss: 2.9605\n",
      "Epoch [242/`300], Step [30/46], Loss: 2.7854\n",
      "Epoch [242/`300], Step [40/46], Loss: 2.9298\n",
      "Epoch [243/`300], Step [10/46], Loss: 2.8804\n",
      "Epoch [243/`300], Step [20/46], Loss: 2.8904\n",
      "Epoch [243/`300], Step [30/46], Loss: 2.7322\n",
      "Epoch [243/`300], Step [40/46], Loss: 2.9298\n",
      "Epoch [244/`300], Step [10/46], Loss: 2.8922\n",
      "Epoch [244/`300], Step [20/46], Loss: 2.8376\n",
      "Epoch [244/`300], Step [30/46], Loss: 2.9561\n",
      "Epoch [244/`300], Step [40/46], Loss: 2.7773\n",
      "Epoch [245/`300], Step [10/46], Loss: 2.8686\n",
      "Epoch [245/`300], Step [20/46], Loss: 2.8893\n",
      "Epoch [245/`300], Step [30/46], Loss: 2.8935\n",
      "Epoch [245/`300], Step [40/46], Loss: 2.8682\n",
      "Epoch [246/`300], Step [10/46], Loss: 2.7522\n",
      "Epoch [246/`300], Step [20/46], Loss: 3.0684\n",
      "Epoch [246/`300], Step [30/46], Loss: 2.7525\n",
      "Epoch [246/`300], Step [40/46], Loss: 3.0238\n",
      "Epoch [247/`300], Step [10/46], Loss: 2.7099\n",
      "Epoch [247/`300], Step [20/46], Loss: 2.7824\n",
      "Epoch [247/`300], Step [30/46], Loss: 2.7914\n",
      "Epoch [247/`300], Step [40/46], Loss: 2.8628\n",
      "Epoch [248/`300], Step [10/46], Loss: 2.9051\n",
      "Epoch [248/`300], Step [20/46], Loss: 2.8748\n",
      "Epoch [248/`300], Step [30/46], Loss: 2.7658\n",
      "Epoch [248/`300], Step [40/46], Loss: 2.8699\n",
      "Epoch [249/`300], Step [10/46], Loss: 2.7930\n",
      "Epoch [249/`300], Step [20/46], Loss: 2.8571\n",
      "Epoch [249/`300], Step [30/46], Loss: 2.7734\n",
      "Epoch [249/`300], Step [40/46], Loss: 2.7131\n",
      "Epoch [250/`300], Step [10/46], Loss: 2.7971\n",
      "Epoch [250/`300], Step [20/46], Loss: 2.8642\n",
      "Epoch [250/`300], Step [30/46], Loss: 2.8678\n",
      "Epoch [250/`300], Step [40/46], Loss: 2.9805\n",
      "Epoch [251/`300], Step [10/46], Loss: 2.9286\n",
      "Epoch [251/`300], Step [20/46], Loss: 2.8135\n",
      "Epoch [251/`300], Step [30/46], Loss: 2.8687\n",
      "Epoch [251/`300], Step [40/46], Loss: 2.9075\n",
      "Epoch [252/`300], Step [10/46], Loss: 2.9111\n",
      "Epoch [252/`300], Step [20/46], Loss: 2.8334\n",
      "Epoch [252/`300], Step [30/46], Loss: 2.7245\n",
      "Epoch [252/`300], Step [40/46], Loss: 2.8422\n",
      "Epoch [253/`300], Step [10/46], Loss: 2.7922\n",
      "Epoch [253/`300], Step [20/46], Loss: 2.8784\n",
      "Epoch [253/`300], Step [30/46], Loss: 2.7653\n",
      "Epoch [253/`300], Step [40/46], Loss: 2.9018\n",
      "Epoch [254/`300], Step [10/46], Loss: 2.7955\n",
      "Epoch [254/`300], Step [20/46], Loss: 2.9117\n",
      "Epoch [254/`300], Step [30/46], Loss: 3.0027\n",
      "Epoch [254/`300], Step [40/46], Loss: 2.8603\n",
      "Epoch [255/`300], Step [10/46], Loss: 2.7474\n",
      "Epoch [255/`300], Step [20/46], Loss: 2.8017\n",
      "Epoch [255/`300], Step [30/46], Loss: 2.7305\n",
      "Epoch [255/`300], Step [40/46], Loss: 2.8630\n",
      "Epoch [256/`300], Step [10/46], Loss: 2.7145\n",
      "Epoch [256/`300], Step [20/46], Loss: 2.9841\n",
      "Epoch [256/`300], Step [30/46], Loss: 2.8972\n",
      "Epoch [256/`300], Step [40/46], Loss: 2.8347\n",
      "Epoch [257/`300], Step [10/46], Loss: 2.8104\n",
      "Epoch [257/`300], Step [20/46], Loss: 3.0361\n",
      "Epoch [257/`300], Step [30/46], Loss: 2.8626\n",
      "Epoch [257/`300], Step [40/46], Loss: 2.9838\n",
      "Epoch [258/`300], Step [10/46], Loss: 2.8490\n",
      "Epoch [258/`300], Step [20/46], Loss: 2.8432\n",
      "Epoch [258/`300], Step [30/46], Loss: 2.8657\n",
      "Epoch [258/`300], Step [40/46], Loss: 2.7239\n",
      "Epoch [259/`300], Step [10/46], Loss: 2.7650\n",
      "Epoch [259/`300], Step [20/46], Loss: 2.7359\n",
      "Epoch [259/`300], Step [30/46], Loss: 2.7894\n",
      "Epoch [259/`300], Step [40/46], Loss: 2.9180\n",
      "Epoch [260/`300], Step [10/46], Loss: 2.8074\n",
      "Epoch [260/`300], Step [20/46], Loss: 2.7105\n",
      "Epoch [260/`300], Step [30/46], Loss: 2.8342\n",
      "Epoch [260/`300], Step [40/46], Loss: 3.0468\n",
      "Epoch [261/`300], Step [10/46], Loss: 3.0432\n",
      "Epoch [261/`300], Step [20/46], Loss: 2.8680\n",
      "Epoch [261/`300], Step [30/46], Loss: 2.8631\n",
      "Epoch [261/`300], Step [40/46], Loss: 2.8416\n",
      "Epoch [262/`300], Step [10/46], Loss: 2.9486\n",
      "Epoch [262/`300], Step [20/46], Loss: 2.9277\n",
      "Epoch [262/`300], Step [30/46], Loss: 2.7792\n",
      "Epoch [262/`300], Step [40/46], Loss: 2.7656\n",
      "Epoch [263/`300], Step [10/46], Loss: 2.9431\n",
      "Epoch [263/`300], Step [20/46], Loss: 2.7415\n",
      "Epoch [263/`300], Step [30/46], Loss: 2.9656\n",
      "Epoch [263/`300], Step [40/46], Loss: 2.8996\n",
      "Epoch [264/`300], Step [10/46], Loss: 2.8411\n",
      "Epoch [264/`300], Step [20/46], Loss: 2.8837\n",
      "Epoch [264/`300], Step [30/46], Loss: 2.7939\n",
      "Epoch [264/`300], Step [40/46], Loss: 2.8765\n",
      "Epoch [265/`300], Step [10/46], Loss: 2.8437\n",
      "Epoch [265/`300], Step [20/46], Loss: 2.7845\n",
      "Epoch [265/`300], Step [30/46], Loss: 2.8382\n",
      "Epoch [265/`300], Step [40/46], Loss: 3.1126\n",
      "Epoch [266/`300], Step [10/46], Loss: 2.8584\n",
      "Epoch [266/`300], Step [20/46], Loss: 2.7057\n",
      "Epoch [266/`300], Step [30/46], Loss: 2.8849\n",
      "Epoch [266/`300], Step [40/46], Loss: 2.7774\n",
      "Epoch [267/`300], Step [10/46], Loss: 2.9218\n",
      "Epoch [267/`300], Step [20/46], Loss: 2.5777\n",
      "Epoch [267/`300], Step [30/46], Loss: 2.8168\n",
      "Epoch [267/`300], Step [40/46], Loss: 2.7878\n",
      "Epoch [268/`300], Step [10/46], Loss: 2.5870\n",
      "Epoch [268/`300], Step [20/46], Loss: 2.7869\n",
      "Epoch [268/`300], Step [30/46], Loss: 2.7601\n",
      "Epoch [268/`300], Step [40/46], Loss: 2.7131\n",
      "Epoch [269/`300], Step [10/46], Loss: 2.8345\n",
      "Epoch [269/`300], Step [20/46], Loss: 2.9610\n",
      "Epoch [269/`300], Step [30/46], Loss: 2.7579\n",
      "Epoch [269/`300], Step [40/46], Loss: 2.8647\n",
      "Epoch [270/`300], Step [10/46], Loss: 2.7502\n",
      "Epoch [270/`300], Step [20/46], Loss: 2.7900\n",
      "Epoch [270/`300], Step [30/46], Loss: 2.8314\n",
      "Epoch [270/`300], Step [40/46], Loss: 3.0227\n",
      "Epoch [271/`300], Step [10/46], Loss: 2.9111\n",
      "Epoch [271/`300], Step [20/46], Loss: 2.8192\n",
      "Epoch [271/`300], Step [30/46], Loss: 2.7771\n",
      "Epoch [271/`300], Step [40/46], Loss: 2.8041\n",
      "Epoch [272/`300], Step [10/46], Loss: 2.7907\n",
      "Epoch [272/`300], Step [20/46], Loss: 2.8701\n",
      "Epoch [272/`300], Step [30/46], Loss: 2.7308\n",
      "Epoch [272/`300], Step [40/46], Loss: 2.7841\n",
      "Epoch [273/`300], Step [10/46], Loss: 2.8006\n",
      "Epoch [273/`300], Step [20/46], Loss: 2.8918\n",
      "Epoch [273/`300], Step [30/46], Loss: 2.7317\n",
      "Epoch [273/`300], Step [40/46], Loss: 2.6978\n",
      "Epoch [274/`300], Step [10/46], Loss: 2.8688\n",
      "Epoch [274/`300], Step [20/46], Loss: 2.7080\n",
      "Epoch [274/`300], Step [30/46], Loss: 2.7298\n",
      "Epoch [274/`300], Step [40/46], Loss: 2.6980\n",
      "Epoch [275/`300], Step [10/46], Loss: 2.9455\n",
      "Epoch [275/`300], Step [20/46], Loss: 2.8340\n",
      "Epoch [275/`300], Step [30/46], Loss: 2.8694\n",
      "Epoch [275/`300], Step [40/46], Loss: 2.9026\n",
      "Epoch [276/`300], Step [10/46], Loss: 2.8106\n",
      "Epoch [276/`300], Step [20/46], Loss: 2.7681\n",
      "Epoch [276/`300], Step [30/46], Loss: 2.8104\n",
      "Epoch [276/`300], Step [40/46], Loss: 2.6778\n",
      "Epoch [277/`300], Step [10/46], Loss: 2.6637\n",
      "Epoch [277/`300], Step [20/46], Loss: 2.7414\n",
      "Epoch [277/`300], Step [30/46], Loss: 2.8423\n",
      "Epoch [277/`300], Step [40/46], Loss: 2.7960\n",
      "Epoch [278/`300], Step [10/46], Loss: 2.8634\n",
      "Epoch [278/`300], Step [20/46], Loss: 2.9390\n",
      "Epoch [278/`300], Step [30/46], Loss: 2.6356\n",
      "Epoch [278/`300], Step [40/46], Loss: 2.7834\n",
      "Epoch [279/`300], Step [10/46], Loss: 2.8995\n",
      "Epoch [279/`300], Step [20/46], Loss: 2.6595\n",
      "Epoch [279/`300], Step [30/46], Loss: 2.9225\n",
      "Epoch [279/`300], Step [40/46], Loss: 2.7859\n",
      "Epoch [280/`300], Step [10/46], Loss: 2.6284\n",
      "Epoch [280/`300], Step [20/46], Loss: 2.6527\n",
      "Epoch [280/`300], Step [30/46], Loss: 2.8467\n",
      "Epoch [280/`300], Step [40/46], Loss: 2.8446\n",
      "Epoch [281/`300], Step [10/46], Loss: 2.8557\n",
      "Epoch [281/`300], Step [20/46], Loss: 2.8955\n",
      "Epoch [281/`300], Step [30/46], Loss: 2.7497\n",
      "Epoch [281/`300], Step [40/46], Loss: 2.7572\n",
      "Epoch [282/`300], Step [10/46], Loss: 2.8621\n",
      "Epoch [282/`300], Step [20/46], Loss: 2.7484\n",
      "Epoch [282/`300], Step [30/46], Loss: 2.8916\n",
      "Epoch [282/`300], Step [40/46], Loss: 2.7823\n",
      "Epoch [283/`300], Step [10/46], Loss: 2.9091\n",
      "Epoch [283/`300], Step [20/46], Loss: 2.7667\n",
      "Epoch [283/`300], Step [30/46], Loss: 2.8835\n",
      "Epoch [283/`300], Step [40/46], Loss: 2.6468\n",
      "Epoch [284/`300], Step [10/46], Loss: 2.6388\n",
      "Epoch [284/`300], Step [20/46], Loss: 2.7069\n",
      "Epoch [284/`300], Step [30/46], Loss: 2.7189\n",
      "Epoch [284/`300], Step [40/46], Loss: 2.8183\n",
      "Epoch [285/`300], Step [10/46], Loss: 2.6911\n",
      "Epoch [285/`300], Step [20/46], Loss: 2.7360\n",
      "Epoch [285/`300], Step [30/46], Loss: 2.7182\n",
      "Epoch [285/`300], Step [40/46], Loss: 2.8547\n",
      "Epoch [286/`300], Step [10/46], Loss: 2.6800\n",
      "Epoch [286/`300], Step [20/46], Loss: 2.5533\n",
      "Epoch [286/`300], Step [30/46], Loss: 2.9958\n",
      "Epoch [286/`300], Step [40/46], Loss: 2.9605\n",
      "Epoch [287/`300], Step [10/46], Loss: 2.7834\n",
      "Epoch [287/`300], Step [20/46], Loss: 2.9831\n",
      "Epoch [287/`300], Step [30/46], Loss: 2.8182\n",
      "Epoch [287/`300], Step [40/46], Loss: 2.8061\n",
      "Epoch [288/`300], Step [10/46], Loss: 2.8512\n",
      "Epoch [288/`300], Step [20/46], Loss: 2.6368\n",
      "Epoch [288/`300], Step [30/46], Loss: 2.7684\n",
      "Epoch [288/`300], Step [40/46], Loss: 2.8182\n",
      "Epoch [289/`300], Step [10/46], Loss: 2.9025\n",
      "Epoch [289/`300], Step [20/46], Loss: 2.7072\n",
      "Epoch [289/`300], Step [30/46], Loss: 2.8585\n",
      "Epoch [289/`300], Step [40/46], Loss: 2.7608\n",
      "Epoch [290/`300], Step [10/46], Loss: 2.7535\n",
      "Epoch [290/`300], Step [20/46], Loss: 3.1409\n",
      "Epoch [290/`300], Step [30/46], Loss: 2.7185\n",
      "Epoch [290/`300], Step [40/46], Loss: 2.7283\n",
      "Epoch [291/`300], Step [10/46], Loss: 2.5738\n",
      "Epoch [291/`300], Step [20/46], Loss: 2.8199\n",
      "Epoch [291/`300], Step [30/46], Loss: 3.0425\n",
      "Epoch [291/`300], Step [40/46], Loss: 2.7587\n",
      "Epoch [292/`300], Step [10/46], Loss: 2.8033\n",
      "Epoch [292/`300], Step [20/46], Loss: 2.8412\n",
      "Epoch [292/`300], Step [30/46], Loss: 2.8521\n",
      "Epoch [292/`300], Step [40/46], Loss: 2.7789\n",
      "Epoch [293/`300], Step [10/46], Loss: 2.7346\n",
      "Epoch [293/`300], Step [20/46], Loss: 2.8964\n",
      "Epoch [293/`300], Step [30/46], Loss: 2.6925\n",
      "Epoch [293/`300], Step [40/46], Loss: 2.7346\n",
      "Epoch [294/`300], Step [10/46], Loss: 2.7252\n",
      "Epoch [294/`300], Step [20/46], Loss: 3.0052\n",
      "Epoch [294/`300], Step [30/46], Loss: 2.6589\n",
      "Epoch [294/`300], Step [40/46], Loss: 2.7775\n",
      "Epoch [295/`300], Step [10/46], Loss: 2.6752\n",
      "Epoch [295/`300], Step [20/46], Loss: 2.7046\n",
      "Epoch [295/`300], Step [30/46], Loss: 2.9186\n",
      "Epoch [295/`300], Step [40/46], Loss: 2.8445\n",
      "Epoch [296/`300], Step [10/46], Loss: 2.6111\n",
      "Epoch [296/`300], Step [20/46], Loss: 2.9196\n",
      "Epoch [296/`300], Step [30/46], Loss: 2.6259\n",
      "Epoch [296/`300], Step [40/46], Loss: 2.9280\n",
      "Epoch [297/`300], Step [10/46], Loss: 2.7521\n",
      "Epoch [297/`300], Step [20/46], Loss: 2.8468\n",
      "Epoch [297/`300], Step [30/46], Loss: 2.6630\n",
      "Epoch [297/`300], Step [40/46], Loss: 2.6803\n",
      "Epoch [298/`300], Step [10/46], Loss: 2.7418\n",
      "Epoch [298/`300], Step [20/46], Loss: 2.9304\n",
      "Epoch [298/`300], Step [30/46], Loss: 2.8039\n",
      "Epoch [298/`300], Step [40/46], Loss: 2.6750\n",
      "Epoch [299/`300], Step [10/46], Loss: 2.7552\n",
      "Epoch [299/`300], Step [20/46], Loss: 2.8577\n",
      "Epoch [299/`300], Step [30/46], Loss: 2.9195\n",
      "Epoch [299/`300], Step [40/46], Loss: 2.8437\n",
      "Epoch [300/`300], Step [10/46], Loss: 2.6911\n",
      "Epoch [300/`300], Step [20/46], Loss: 2.7115\n",
      "Epoch [300/`300], Step [30/46], Loss: 2.6252\n",
      "Epoch [300/`300], Step [40/46], Loss: 2.9172\n"
     ]
    }
   ],
   "source": [
    "model = InceptionTime(1, close_num + 1)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (signals, labels) in enumerate(train_loader):\n",
    "        signals = signals.float()\n",
    "        signals = signals.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(signals)\n",
    "        outputs = outputs.to(device)\n",
    "        loss = triple_joint_loss(outputs, labels, alpha)\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_centloss.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in center_loss.parameters():\n",
    "            param.grad.data *= (1./alpha)\n",
    "        optimizer.step()\n",
    "        optimizer_centloss.step()\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/`{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBase([0.3516, 0.2865, 0.4539, 0.6261, 0.3036, 0.2910, 0.1780, 0.6148,\n",
      "            0.5123, 0.2918, 0.3540, 0.5016, 0.1934, 0.2702, 0.3709, 0.2174,\n",
      "            0.1873, 0.1379, 0.2172, 0.1962, 0.2103, 0.1701, 0.2517, 0.2473,\n",
      "            0.1397, 0.2121, 0.8141, 0.1830, 0.9476, 0.2071, 0.2550, 0.1653,\n",
      "            0.1621, 0.2120, 0.2817, 0.2091, 0.2555, 0.1616, 0.4868, 0.2876,\n",
      "            0.5577, 0.2495, 0.5113, 0.5363, 0.1565, 0.1919, 0.3597, 0.4712,\n",
      "            0.8728, 0.2886, 0.1452, 0.3193, 0.3994, 0.1945, 0.2081, 0.5702,\n",
      "            0.1710, 0.5271, 0.4434, 0.5169, 0.7126, 0.1736, 0.3862, 0.4264],\n",
      "           device='cuda:0') TensorBase([ 0,  0,  4,  0,  6, 11,  8,  0,  0,  0,  1,  9,  1,  1,  0,  0,  0,\n",
      "            11,  3,  5,  0,  3, 11, 11, 10,  0,  4, 11,  4,  7,  4,  0,  3,  0,\n",
      "             0,  6,  1,  8,  5,  0,  4,  1,  2,  0,  3,  0,  5, 11,  0, 11,  3,\n",
      "             0,  4,  5, 11,  0, 10, 11,  9,  0,  0,  0,  0,  0],\n",
      "           device='cuda:0') tensor([ 0,  1,  4,  0,  1,  7,  3,  0,  0,  0,  5,  9,  1,  0,  0,  6,  7,  0,\n",
      "         3,  1,  7,  6,  5,  1,  3,  6,  4, 11,  4,  0,  4,  8, 10,  7,  8,  1,\n",
      "         5,  7,  5,  0,  4,  5,  2,  8, 10, 10,  5, 11,  0, 11,  3,  8,  4,  3,\n",
      "        11,  8,  3, 11,  9,  0,  0,  1,  2,  8], device='cuda:0')\n",
      "29 / 64 = Acc: 45.3125 %\n",
      "TensorBase([0.4302, 0.1907, 0.5572, 0.1984, 0.2059, 0.2089, 0.4522, 0.2592,\n",
      "            0.2371, 0.6376, 0.1436, 0.3251, 0.2424, 0.2108, 0.2284, 0.2767,\n",
      "            0.3042, 0.5985, 0.4043, 0.3301, 0.3372, 0.2161, 0.6560, 0.2355,\n",
      "            0.3694, 0.3637, 0.3097, 0.4105, 0.4135, 0.3794, 0.1955, 0.1688,\n",
      "            0.4626, 0.1862, 0.5073, 0.2195, 0.1855, 0.2297, 0.3035, 0.1876,\n",
      "            0.8871, 0.1852, 0.1719, 0.2469, 0.5892, 0.3049, 0.1547, 0.2364,\n",
      "            0.4204, 0.2868, 0.1611, 0.4424, 0.3306, 0.1514, 0.1627, 0.3506,\n",
      "            0.2115, 0.3769, 0.1403, 0.1518, 0.7865, 0.5315, 0.1330, 0.3694],\n",
      "           device='cuda:0') TensorBase([ 0, 11,  0, 11,  8,  8,  2, 11,  2,  9,  2,  0,  0,  5, 11,  3,  0,\n",
      "             0,  0,  9,  5,  2,  0,  0,  0,  4,  1,  4,  6,  0,  5,  0,  0,  1,\n",
      "             0,  0, 11,  8,  1,  0,  4,  6,  2,  3,  0, 11,  6,  1,  0,  0,  8,\n",
      "             4,  1,  6,  3, 11,  1,  4,  6,  1,  9,  0,  4,  1],\n",
      "           device='cuda:0') tensor([ 0,  7,  0,  3,  8,  8,  2, 11,  2,  9,  2,  7,  0,  3,  7, 10,  8,  0,\n",
      "         0,  9,  5,  2,  0,  7,  0,  4,  1,  4, 11,  0,  5,  3,  0,  1,  0,  7,\n",
      "         9,  0,  5,  1,  4,  6, 10,  7,  0,  6,  6, 11,  2,  0,  8,  4,  1,  1,\n",
      "         5,  3, 10,  4, 10,  1,  9, 10,  7,  1], device='cuda:0')\n",
      "66 / 128 = Acc: 51.5625 %\n",
      "TensorBase([0.2929, 0.3593, 0.3955, 0.1637, 0.3928, 0.6961, 0.4373, 0.3982,\n",
      "            0.1500, 0.1389, 0.2359, 0.2627, 0.2406, 0.3231, 0.2440, 0.2642,\n",
      "            0.9814, 0.4253, 0.1425, 0.5845, 0.2519, 0.9109, 0.1645, 0.3480,\n",
      "            0.3687, 0.2509, 0.2020, 0.6172, 0.1662, 0.1607, 0.1407, 0.2260,\n",
      "            0.6710, 0.1430, 0.5100, 0.2638, 0.7369, 0.8150, 0.1909, 0.1950,\n",
      "            0.5046, 0.7136, 0.3540, 0.1889, 0.2749, 0.1926, 0.3996, 0.2415,\n",
      "            0.5010, 0.1968, 0.3363, 0.3287, 0.1540, 0.2432, 0.3068, 0.2449,\n",
      "            0.4638, 0.2783, 0.2909, 0.2715, 0.2239, 0.2709, 0.3172, 0.1343],\n",
      "           device='cuda:0') TensorBase([ 6,  0, 11,  3, 11,  0,  2,  2,  8,  3, 11,  5,  8, 11,  0,  0,  4,\n",
      "            11, 11,  2,  0,  0, 11,  0,  0,  1,  1,  9,  1,  6,  0,  1,  9,  3,\n",
      "             9,  4,  9,  9,  0,  8,  0,  0,  5, 11,  5,  3,  2,  5,  2,  0,  2,\n",
      "             0,  5,  0,  1, 11,  9, 11,  0,  6,  0,  1,  9,  0],\n",
      "           device='cuda:0') tensor([11,  0, 11,  5,  6,  0,  2,  2,  3,  7, 11, 10,  8,  7,  3,  0,  4, 10,\n",
      "         5,  2, 10,  0, 11,  0,  0,  1,  7,  9,  8,  6,  0, 11,  9, 10,  3,  4,\n",
      "         9,  9,  2, 10,  0,  0,  5,  5,  5,  4,  2,  5,  2,  8,  2,  0, 10,  3,\n",
      "        11, 11,  9,  1,  0, 11,  0,  1,  9,  7], device='cuda:0')\n",
      "103 / 192 = Acc: 53.645833333333336 %\n",
      "TensorBase([0.1549, 0.4443, 0.2565, 0.5980, 0.2338, 0.1794, 0.3028, 0.2197,\n",
      "            0.9550, 0.1663, 0.3593, 0.3475, 0.1753, 0.4682, 0.7019, 0.9573,\n",
      "            0.2463, 0.2011, 0.3816, 0.2006, 0.2259, 0.7879, 0.1521, 0.3291,\n",
      "            0.3277, 0.6210, 0.1734, 0.3091, 0.4938, 0.3657, 0.2848, 0.3039,\n",
      "            0.2547, 0.3072, 0.3916, 0.7107, 0.3661, 0.3932, 0.2559, 0.3892,\n",
      "            0.2047, 0.1962, 0.2738, 0.2666, 0.4478, 0.2845, 0.1945, 0.3279,\n",
      "            0.1720, 0.2499, 0.2958, 0.1608, 0.1593, 0.2582, 0.6658, 0.4935,\n",
      "            0.3024, 0.3447, 0.3176, 0.5179, 0.3830, 0.2297, 0.8117, 0.2918],\n",
      "           device='cuda:0') TensorBase([ 0, 11,  0,  4,  5,  5, 11, 11,  4, 11,  0,  0,  8, 11,  9,  4,  1,\n",
      "             6, 11,  0, 10,  4,  3, 11, 11,  0, 11,  0,  0,  2,  0,  0,  0, 11,\n",
      "            11,  9, 11,  0,  0, 11,  6,  1,  1, 11,  2,  6,  3,  0, 11,  6,  0,\n",
      "             0, 11,  0,  9,  0,  0, 11, 11,  0,  0,  0,  2,  4],\n",
      "           device='cuda:0') tensor([ 8, 11,  8,  4,  5,  5, 10,  7,  4,  1,  0,  8, 10, 11,  9,  4,  1,  3,\n",
      "         6,  8, 10,  4,  6, 11, 11,  0,  4,  5,  0,  2,  8,  0,  3, 11, 11,  9,\n",
      "        11,  0,  2,  1,  7,  1,  1,  5,  2,  6,  7,  8,  3, 11,  2, 10,  3, 11,\n",
      "         9,  0,  8,  1,  1,  0,  0, 10,  2,  9], device='cuda:0')\n",
      "135 / 256 = Acc: 52.734375 %\n",
      "TensorBase([0.2571, 0.5173, 0.3761, 0.8533, 0.4414, 0.2730, 0.2086, 0.4674,\n",
      "            0.1517, 0.1539, 0.1483, 0.5165, 0.3916, 0.2232, 0.2346, 0.5907,\n",
      "            0.1930, 0.2293, 0.2169, 0.2030, 0.8595, 0.3099, 0.1592, 0.2503,\n",
      "            0.4464, 0.4248, 0.8048, 0.4438, 0.4838, 0.6946, 0.3333, 0.1659,\n",
      "            0.2713, 0.3871, 0.2110, 0.4983, 0.2370, 0.2687, 0.2074, 0.3365,\n",
      "            0.2914, 0.1839, 0.2506, 0.2858, 0.3873, 0.2688, 0.9025, 0.3904,\n",
      "            0.6448, 0.8430, 0.6294, 0.1617, 0.1611, 0.3212, 0.3731, 0.3295,\n",
      "            0.2867, 0.2605, 0.4950, 0.3118, 0.3063, 0.2553, 0.2542, 0.1724],\n",
      "           device='cuda:0') TensorBase([ 0,  0,  0,  4,  4,  0,  8,  4,  0,  0,  0,  0, 11,  1,  0,  4,  0,\n",
      "             6,  0,  1,  4, 11, 11,  2,  2,  9,  0, 11,  9,  0, 11,  0, 11,  2,\n",
      "             0,  9,  0,  0,  0,  6, 11,  0,  5,  0, 11,  1,  4,  4,  4,  4,  4,\n",
      "             0, 11,  5,  0,  5,  0,  0, 11, 11,  0,  9, 11,  1],\n",
      "           device='cuda:0') tensor([ 0,  0,  2,  4,  4, 10,  3,  4,  1,  1, 11,  0,  9,  1,  0,  4,  7,  6,\n",
      "         6,  7,  4,  7,  1,  2,  2,  9,  0, 11,  9,  0, 11,  7, 11,  2, 10,  9,\n",
      "         5,  7,  0,  1, 11,  0,  5,  1, 11, 11,  4,  4,  4,  4,  4,  5, 10,  6,\n",
      "         0,  6, 10,  8,  1,  6,  0,  9,  7,  1], device='cuda:0')\n",
      "171 / 320 = Acc: 53.4375 %\n",
      "TensorBase([0.1399, 0.2080, 0.2930, 0.5737, 0.1875, 0.1346, 0.2130, 0.2002,\n",
      "            0.2357, 0.1982, 0.3538, 0.5501, 0.3170, 0.5284, 0.1835, 0.4110,\n",
      "            0.4757, 0.3175, 0.3048, 0.2586, 0.3636, 0.1691, 0.2426, 0.1872,\n",
      "            0.8803, 0.1399, 0.3481, 0.3296, 0.3730, 0.3061, 0.1809, 0.1499,\n",
      "            0.3550, 0.2041, 0.2162, 0.1899, 0.1879, 0.3139, 0.4024, 0.1840,\n",
      "            0.1842, 0.2360, 0.1839, 0.2488, 0.6721, 0.4848, 0.3005, 0.3185,\n",
      "            0.3298, 0.2135, 0.2524, 0.4072, 0.1502, 0.4109, 0.2372, 0.1575,\n",
      "            0.3479, 0.2146, 0.3634, 0.1539, 0.1674, 0.2003, 0.2589, 0.4502],\n",
      "           device='cuda:0') TensorBase([ 0,  1,  0,  0,  9,  3,  5,  0,  9,  2,  5,  4,  0,  0, 11,  0,  0,\n",
      "             0,  6,  0, 11,  1,  1,  5,  0,  3, 11,  6,  0,  0,  1, 11,  9,  0,\n",
      "             0,  8,  0,  1,  9,  0,  0, 11,  0,  5,  9,  0,  0,  0, 11,  0, 11,\n",
      "             9,  7,  0,  1,  3,  0,  0,  4,  2,  1,  0,  0,  0],\n",
      "           device='cuda:0') tensor([ 8,  7, 10,  0,  9,  6,  5,  7,  9,  3,  5,  4,  0,  8,  6,  3,  8,  7,\n",
      "        11,  5,  6,  1,  1, 10,  0,  7,  4,  6,  8,  6,  1, 11,  9,  2,  0,  6,\n",
      "         8,  7,  9,  0,  8, 11,  8,  5,  9,  0,  7,  7,  7,  5,  9,  7,  7,  8,\n",
      "         0,  8, 10,  1,  4, 11,  7,  8, 10,  0], device='cuda:0')\n",
      "195 / 384 = Acc: 50.78125 %\n",
      "TensorBase([0.3768, 0.1960, 0.3225, 0.3371, 0.6698, 0.3007, 0.6987, 0.9781,\n",
      "            0.1541, 0.1846, 0.2109, 0.2515, 0.2395, 0.2481, 0.1161, 0.8179,\n",
      "            0.2309, 0.1995, 0.3072, 0.1653, 0.3237, 0.2702, 0.1652, 0.1391,\n",
      "            0.2808, 0.8075, 0.2394, 0.1906, 0.1521, 0.3865, 0.3607, 0.2679,\n",
      "            0.3221, 0.3200, 0.3920, 0.1282, 0.1884, 0.1775, 0.1593, 0.4926,\n",
      "            0.2725, 0.2361, 0.2696, 0.2042, 0.2019, 0.1843, 0.3267, 0.3298,\n",
      "            0.7317, 0.6398, 0.2167, 0.3000, 0.1548, 0.1489, 0.9170, 0.1591,\n",
      "            0.5538, 0.2023, 0.2034, 0.1856, 0.3992, 0.1802, 0.4748, 0.2221],\n",
      "           device='cuda:0') TensorBase([ 0,  8,  3,  0,  9, 11,  0,  4,  1,  0, 11,  1,  1, 11, 11,  4,  0,\n",
      "             3,  0, 11, 11, 11,  8,  5,  5,  9, 11,  0,  0, 11,  0,  2,  0,  4,\n",
      "             3,  3,  2, 11,  0,  0,  0,  1, 11, 11,  3, 10, 11,  0,  4,  9,  8,\n",
      "             4,  0, 11,  0,  1, 11,  6,  0,  0,  5, 11,  0, 11],\n",
      "           device='cuda:0') tensor([ 0, 10,  1,  0,  9,  1,  0,  4,  9,  2,  7,  1,  1,  1,  8,  4,  5,  7,\n",
      "         0,  5,  1,  6,  3,  3,  5,  9,  7,  6, 10, 11,  2,  9,  0,  1,  3, 10,\n",
      "         3,  6,  8,  0,  0,  7,  3,  9,  3, 10,  1,  0,  4,  9,  8,  4,  5, 11,\n",
      "         0,  5, 11,  6, 10, 10,  5,  3,  0,  7], device='cuda:0')\n",
      "224 / 448 = Acc: 50.0 %\n",
      "TensorBase([0.2291, 0.2118, 0.1854, 0.4031, 0.7051, 0.6748, 0.8889, 0.3845,\n",
      "            0.2796, 0.2506, 0.1922, 0.1488, 0.5675, 0.1711, 0.1603, 0.1271,\n",
      "            0.2541, 0.2067, 0.3249, 0.1901, 0.4702, 0.1836, 0.5192, 0.3173,\n",
      "            0.3043, 0.7337, 0.5316, 0.3022, 0.3403, 0.2363, 0.2670, 0.1875,\n",
      "            0.2776, 0.4445, 0.2028, 0.3113, 0.6435, 0.3412, 0.1554, 0.3164,\n",
      "            0.8550, 0.4427, 0.9568, 0.1734, 0.1436, 0.2535, 0.2789, 0.1996,\n",
      "            0.1676, 0.3159, 0.2309, 0.6006, 0.2459, 0.1437, 0.2997, 0.2011,\n",
      "            0.5300, 0.2692, 0.2997, 0.5321, 0.2968, 0.2413, 0.2656, 0.3020],\n",
      "           device='cuda:0') TensorBase([ 1, 11,  2, 11,  4,  4,  4,  0,  5,  6,  0,  5,  9,  3,  1,  0,  9,\n",
      "             8,  1,  1,  0,  5,  0,  2,  0,  2,  0, 11, 11, 11,  0,  7, 11, 11,\n",
      "             0,  0,  2,  0,  1,  0,  4,  4,  4,  0,  3,  4,  6,  0,  0,  0,  0,\n",
      "             0,  6,  0,  0, 11,  4,  9,  0,  0, 11,  1, 11,  9],\n",
      "           device='cuda:0') tensor([11,  9,  2,  9,  4,  4,  4,  7,  5,  3,  7, 11,  9,  7,  6,  1, 11,  8,\n",
      "         1,  0,  8,  7,  0,  2,  5,  2,  0,  1,  1,  5,  1,  2,  1,  3,  0,  0,\n",
      "         2,  2,  3,  3,  4,  4,  4,  6,  2,  4,  4,  0,  1,  3, 10,  0,  6,  6,\n",
      "         3, 11,  9,  7,  6,  0,  1, 11,  1,  9], device='cuda:0')\n",
      "249 / 512 = Acc: 48.6328125 %\n",
      "TensorBase([0.2655, 0.3369, 0.1562, 0.4958, 0.4000, 0.1467, 0.1772, 0.2728,\n",
      "            0.6851, 0.2834, 0.2379, 0.3834, 0.5269, 0.7696, 0.3580, 0.2948,\n",
      "            0.2412, 0.2478, 0.1786, 0.2752, 0.2178, 0.4570, 0.1903, 0.1504,\n",
      "            0.1452, 0.4128, 0.3615, 0.1781, 0.1333, 0.2566, 0.3925, 0.2713,\n",
      "            0.3513, 0.1633, 0.7625, 0.2095, 0.2727, 0.2792, 0.5925, 0.4491,\n",
      "            0.2416, 0.1765, 0.3220, 0.4449, 0.1309, 0.3077, 0.2243, 0.1962,\n",
      "            0.5209, 0.3230, 0.6315, 0.1274, 0.6530, 0.5130, 0.3107, 0.1725,\n",
      "            0.2846, 0.3165, 0.2770, 0.3817, 0.3703, 0.1420, 0.3319, 0.3543],\n",
      "           device='cuda:0') TensorBase([ 0, 11,  6,  0, 11,  0,  8,  0,  4, 11,  4,  0,  0,  4,  0, 11,  0,\n",
      "            11,  1,  5,  0,  2,  3,  0,  0,  4,  2,  0,  1,  5,  0,  1,  0, 11,\n",
      "             4,  0, 11,  4,  0,  0,  0,  2,  6, 11,  3,  1,  0,  0,  9, 11,  0,\n",
      "             3,  2, 11,  0,  1, 11,  0,  8,  0,  1,  7,  2,  0],\n",
      "           device='cuda:0') tensor([ 2, 11,  1,  1,  0,  1, 10,  3,  4,  9,  7,  8,  1,  9,  0,  6,  2, 10,\n",
      "        11,  5,  0,  2,  7,  2,  6,  4,  2,  5,  7,  5,  0,  1,  7,  1,  4,  6,\n",
      "        11,  1,  0,  0,  1,  3, 11, 11,  5,  5,  1,  8,  9,  1,  0,  4,  2, 11,\n",
      "         9,  0,  4,  0,  8,  0,  0,  7,  6, 10], device='cuda:0')\n",
      "273 / 576 = Acc: 47.395833333333336 %\n",
      "TensorBase([0.3459, 0.3217, 0.4065, 0.3154, 0.3336, 0.5748, 0.2053, 0.4337,\n",
      "            0.4319, 0.3078, 0.4580, 0.1606, 0.2470, 0.3066, 0.2962, 0.7215,\n",
      "            0.1624, 0.4321, 0.2713, 0.2154, 0.1896, 0.5042, 0.3744, 0.2012,\n",
      "            0.4546, 0.2942, 0.2250, 0.1688, 0.6458, 0.1807, 0.3226, 0.2121,\n",
      "            0.1720, 0.6336, 0.2461, 0.2502, 0.1475, 0.6849, 0.1526, 0.2805,\n",
      "            0.1560, 0.2107, 0.4176, 0.3126, 0.3816, 0.2071, 0.2878, 0.9281,\n",
      "            0.1634, 0.1202, 0.5152, 0.2166, 0.6629, 0.1484, 0.4533, 0.1338,\n",
      "            0.8458, 0.9234, 0.2151, 0.1667, 0.2332, 0.1821, 0.2636, 0.4628],\n",
      "           device='cuda:0') TensorBase([11,  0, 11,  0,  0,  4,  0, 11,  4, 11,  2,  0,  0,  0, 11,  4,  1,\n",
      "             0,  9,  0, 11,  4,  0,  2,  0,  3,  8,  5,  4,  0,  4,  4, 11,  2,\n",
      "            11,  2,  3,  0,  6,  0,  0,  0, 11,  5,  0,  0, 11,  4,  0, 11, 11,\n",
      "             0,  4,  4,  2,  4,  4,  9,  2,  0,  6,  0,  0,  0],\n",
      "           device='cuda:0') tensor([10,  2, 11,  7, 10,  1, 10, 10,  4,  1,  2,  5, 10, 10, 11,  4, 11,  0,\n",
      "         9,  5, 11,  4,  8,  3,  0,  2, 10,  1,  2,  1,  2,  9, 10,  2, 11,  2,\n",
      "        10,  0, 10, 10,  2,  3, 11,  5,  0,  2,  9,  4,  8,  3,  1,  3,  4, 11,\n",
      "         2, 10,  4,  9,  8,  7, 11,  5,  1,  7], device='cuda:0')\n",
      "295 / 640 = Acc: 46.09375 %\n",
      "TensorBase([0.1651, 0.3019, 0.4403, 0.6307, 0.2898, 0.2560, 0.2457, 0.3284,\n",
      "            0.1946, 0.2087, 0.1992, 0.7546, 0.1782, 0.2440, 0.1321, 0.2431,\n",
      "            0.7219, 0.2501, 0.1932, 0.2753, 0.2610, 0.3194, 0.2152, 0.4622,\n",
      "            0.4439, 0.3262, 0.3040, 0.3137, 0.4259, 0.3473, 0.1860, 0.6012,\n",
      "            0.3189, 0.2158, 0.2221, 0.2477, 0.2144, 0.3508, 0.2097, 0.9313,\n",
      "            0.6125, 0.3020, 0.3755, 0.3164, 0.7773, 0.2654, 0.2239, 0.2591,\n",
      "            0.1642, 0.2973, 0.1549, 0.2301, 0.4133, 0.1920, 0.2847, 0.5034,\n",
      "            0.3214, 0.2286, 0.1760, 0.2293, 0.1766, 0.2008, 0.9050, 0.3246],\n",
      "           device='cuda:0') TensorBase([ 7,  0,  1,  9,  1,  5, 11,  0,  0,  0, 11,  4,  3,  0,  7,  0,  9,\n",
      "             0,  8, 11,  2,  2, 11,  4,  0,  2,  0,  5,  4, 11,  0,  5,  0,  0,\n",
      "            11,  1,  8,  0, 11,  4,  0, 11,  0,  0,  4,  0,  0,  0, 11,  1,  2,\n",
      "            11,  0, 11,  0, 11,  0,  0, 11, 11,  0,  1,  9, 11],\n",
      "           device='cuda:0') tensor([ 7,  0,  1,  9, 11,  5,  9,  8,  1, 11, 11,  4,  0,  2,  7,  6,  9,  7,\n",
      "         7,  7,  2,  2, 11,  4,  0,  3,  0,  5,  9,  1,  0,  5, 10,  6, 11,  1,\n",
      "         8,  8,  0,  4, 10, 11,  0,  7,  4,  8,  0,  6, 11,  1,  3,  1,  0,  7,\n",
      "         0,  1,  0,  8,  1, 11,  0,  1,  9,  9], device='cuda:0')\n",
      "330 / 704 = Acc: 46.875 %\n",
      "TensorBase([0.1976, 0.4075, 0.3799, 0.3984, 0.7083, 0.4444, 0.2222, 0.3446,\n",
      "            0.2171, 0.1392, 0.1734, 0.6246, 0.7965, 0.1497, 0.2185, 0.9532,\n",
      "            0.8179, 0.2482, 0.2050, 0.4078, 0.6223, 0.6675, 0.1252, 0.7051,\n",
      "            0.1943, 0.2142, 0.1638, 0.8193], device='cuda:0') TensorBase([ 0, 11, 11,  0,  9,  0,  8,  5,  0, 11,  0,  9,  0,  0,  0,  4,  0,\n",
      "             0,  5, 11,  4,  4,  3,  4,  6,  1,  3,  0], device='cuda:0') tensor([ 1, 11, 11,  8,  9,  8, 10,  5,  3, 11, 11,  9,  0,  3,  1,  4,  0,  0,\n",
      "         5,  1,  4,  4,  1,  4, 11,  0,  3,  0], device='cuda:0')\n",
      "346 / 732 = Acc: 47.26775956284153 %\n"
     ]
    }
   ],
   "source": [
    "# For Confusion Matrix\n",
    "predicted_lists = np.zeros(0, dtype=np.int64)\n",
    "one_hot_labels_list = np.zeros(0, dtype=np.int64)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  n_correct = 0\n",
    "  n_samples = 0\n",
    "  softmax = nn.Softmax()\n",
    "  for i, (signals, one_hot_labels) in enumerate(test_loader):\n",
    "    signals = signals.float()\n",
    "    signals = signals.to(device)\n",
    "    one_hot_labels = one_hot_labels.to(device)\n",
    "    outputs = model(signals)\n",
    "    for j, out in enumerate(outputs):\n",
    "      outputs[j] = softmax(out)\n",
    "    _, predicted = torch.max(outputs.data, 1) # predicted per batch size\n",
    "    \"\"\"\n",
    "    Opensetのためのthreshold-softmax\n",
    "    for idx in range(len(_)):\n",
    "      if _[idx] < threshold:\n",
    "        predicted[idx] = Unknown_label # 15, 20, 25\n",
    "    \"\"\"\n",
    "    print(_, predicted, one_hot_labels)\n",
    "    n_samples += one_hot_labels.size(0) # add batch_size\n",
    "    n_correct += (predicted == one_hot_labels).sum().item()\n",
    "    \n",
    "    predicted_cp = predicted.to('cpu').detach().numpy().copy()\n",
    "    one_hot_labels_cp = one_hot_labels.to('cpu').detach().numpy().copy()\n",
    "    predicted_lists = np.concatenate([predicted_lists, predicted_cp])\n",
    "    one_hot_labels_list = np.concatenate([one_hot_labels_list, one_hot_labels_cp])\n",
    "    \n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'{n_correct} / {n_samples} = Acc: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(77.92222222222227, 0.5, 'Ground Truth')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(one_hot_labels_list, predicted_lists)\n",
    "sns.heatmap(cm, square=True, cbar=True, annot=True, cmap='Blues')\n",
    "plt.xlabel(\"Predicted Label\", fontsize=13)\n",
    "plt.ylabel(\"Ground Truth\", fontsize=13)\n",
    "# fig_name = \"sklearn_confusion_matrix_{}_{}.png\".format(Unknown_label, threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5 (default, Nov 23 2021, 15:27:38) \n[GCC 9.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3f59b86193daf02ac44c2d7d891a49d755eb44400e9ea36eaea4c9328767f1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
