{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from tsai.imports import *\n",
    "from tsai.models.layers import *\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy import signal\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 12\n",
    "close_num = 12\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 300\n",
    "sequence_len = 1000 * 5 # sampling_rate * second\n",
    "overlap = int(sequence_len * 0.3)\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "scaler = MinMaxScaler((0, 1))\n",
    "\n",
    "for i in range(1, num_class + 1):\n",
    "    file_path_fil = \"./data/BPF/filterd%02d.csv\" % i\n",
    "    file_path_unfil = \"./data/BPF/unfilterd%02d.csv\" % i\n",
    "    data_fil = pd.read_csv(file_path_fil)\n",
    "    data_np_fil = data_fil.to_numpy().flatten()\n",
    "    # df_fil = pd.DataFrame(data_fil)\n",
    "    # data_unfil = pd.read_csv(file_path_unfil)\n",
    "    # df_unfil = pd.DataFrame(data_unfil)\n",
    "\n",
    "    end = len(data_np_fil)\n",
    "    n = 0\n",
    "    n_stop = sequence_len\n",
    "    data_segs = []\n",
    "    while n_stop < end:\n",
    "        n_start = 0 + ((sequence_len - 1) - (overlap - 1)) * n\n",
    "        n_stop = n_start + sequence_len\n",
    "        tmp = []\n",
    "        seg = data_np_fil[n_start:n_stop].copy()\n",
    "        data_segs.append([seg])\n",
    "        n += 1\n",
    "\n",
    "    data_list.append(data_segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.37779185, 0.37960583, 0.38139324, ..., 0.00149465, 0.00150279,\n",
       "        0.00151537])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[4][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i in range(len(data_list)):\n",
    "    for j in range(len(data_list[i])):\n",
    "        if i <= close_num:\n",
    "            labels.append(i)\n",
    "        else:\n",
    "            labels.append(close_num + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19013/2521482540.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  data_tensor_list = torch.tensor(data_series_list)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([3657, 1, 5000]), torch.Size([3657]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df_list = pd.DataFrame(data_list)\n",
    "tmp = data_df_list.to_numpy().flatten().copy()\n",
    "data_series_list = pd.Series(tmp).dropna()\n",
    "data_np_list = data_series_list.to_numpy().flatten() # tmp => data_np_list\n",
    "labels_np = np.array(labels)\n",
    "for i in reversed(range(len(data_np_list))):\n",
    "    if len(data_np_list[i][0]) != sequence_len: # 決まった長さでないといけない\n",
    "        data_np_list = np.delete(data_np_list, i)\n",
    "        labels_np = np.delete(labels_np, i)\n",
    "data_series_list = pd.Series(data_np_list)\n",
    "labels_series = pd.Series(labels_np)\n",
    "data_tensor_list = torch.tensor(data_series_list)\n",
    "labels_tensor = torch.tensor(labels_series)\n",
    "data_tensor_list.shape, labels_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionModule(Module):\n",
    "    def __init__(self, ni, nf, ks=40, bottleneck=True):\n",
    "        ks = [ks // (2**i) for i in range(3)]\n",
    "        ks = [k if k % 2 != 0 else k - 1 for k in ks]  # ensure odd ks\n",
    "        bottleneck = bottleneck if ni > 1 else False\n",
    "        self.bottleneck = Conv1d(ni, nf, 1, bias=False) if bottleneck else noop\n",
    "        self.convs = nn.ModuleList([Conv1d(nf if bottleneck else ni, nf, k, bias=False) for k in ks])\n",
    "        self.maxconvpool = nn.Sequential(*[nn.MaxPool1d(3, stride=1, padding=1), Conv1d(ni, nf, 1, bias=False)])\n",
    "        self.concat = Concat()\n",
    "        self.bn = BN1d(nf * 4)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_tensor = x\n",
    "        x = self.bottleneck(input_tensor)\n",
    "        x = self.concat([l(x) for l in self.convs] + [self.maxconvpool(input_tensor)])\n",
    "        return self.act(self.bn(x))\n",
    "\n",
    "\n",
    "@delegates(InceptionModule.__init__)\n",
    "class InceptionBlock(Module):\n",
    "    def __init__(self, ni, nf=32, residual=True, depth=6, **kwargs):\n",
    "        self.residual, self.depth = residual, depth\n",
    "        self.inception, self.shortcut = nn.ModuleList(), nn.ModuleList()\n",
    "        for d in range(depth):\n",
    "            self.inception.append(InceptionModule(ni if d == 0 else nf * 4, nf, **kwargs))\n",
    "            if self.residual and d % 3 == 2: \n",
    "                n_in, n_out = ni if d == 2 else nf * 4, nf * 4\n",
    "                self.shortcut.append(BN1d(n_in) if n_in == n_out else ConvBlock(n_in, n_out, 1, act=None))\n",
    "        self.add = Add()\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        for d, l in enumerate(range(self.depth)):\n",
    "            x = self.inception[d](x)\n",
    "            if self.residual and d % 3 == 2: res = x = self.act(self.add(x, self.shortcut[d//3](res)))\n",
    "        return x\n",
    "\n",
    "    \n",
    "@delegates(InceptionModule.__init__)\n",
    "class InceptionTime(Module):\n",
    "    def __init__(self, c_in, c_out, seq_len=None, nf=32, nb_filters=None, **kwargs):\n",
    "        nf = ifnone(nf, nb_filters) # for compatibility\n",
    "        self.inceptionblock = InceptionBlock(c_in, nf, **kwargs) # c_in is input channel num of conv1d\n",
    "        self.gap = GAP1d(1)\n",
    "        self.fc = nn.Linear(nf * 4, c_out) # c_out is 1d output size \n",
    "        self.fc_tsne = nn.Linear(nf * 4, 2)\n",
    "        self.two_vecs_train = [] # list is faster in appending\n",
    "        self.two_vecs_test = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.inceptionblock(x)\n",
    "        x = self.gap(x)\n",
    "        two_dimensional_vec = self.fc_tsne(x)\n",
    "        if self.training:\n",
    "            self.two_vecs_train.append(two_dimensional_vec.tolist()) # あとでネストしたものをまとめてtensorかndarrayに変換するため\n",
    "        else:\n",
    "            self.two_vecs_test.append(two_dimensional_vec.tolist())\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HCU_Dataset(Dataset):\n",
    "    def __init__(self, dataset, labels) -> None:\n",
    "        # super().__init__()\n",
    "        self.radar_heartbeat = dataset\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "          idx = idx.tolist()        \n",
    "        return self.radar_heartbeat[idx], self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.radar_heartbeat)\n",
    "\n",
    "\n",
    "dataset = HCU_Dataset(data_tensor_list, labels_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットでラベルが変わる区切りを発見する(openset用)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0105, 0.0106, 0.0106,  ..., 0.0023, 0.0022, 0.0022]],\n",
       "        dtype=torch.float64),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### close setの場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set: 2925, Test Set: 732\n"
     ]
    }
   ],
   "source": [
    "close_train_size = int(0.80 * len(dataset))\n",
    "close_test_size = len(dataset) - close_train_size\n",
    "close_train_set, close_test_set = torch.utils.data.random_split(dataset, [close_train_size, close_test_size])\n",
    "print(f\"Train Set: {len(close_train_set)}, Test Set: {len(close_test_set)}\")\n",
    "train_loader = DataLoader(dataset=close_train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=close_test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import log\n",
    "softmax = nn.Softmax()\n",
    "\n",
    "def softmax_loss(outputs, labels):\n",
    "    loss = 0\n",
    "    batch_size = len(labels)\n",
    "    logsoftmax_out = log(softmax(outputs))\n",
    "    for idx in range(batch_size):\n",
    "        loss += 1.0 - logsoftmax_out[idx][labels[idx]]\n",
    "    \n",
    "    return loss / batch_size\n",
    "\n",
    "\n",
    "from center_loss import CenterLoss\n",
    "center_loss = CenterLoss(num_classes=close_num + 1, feat_dim=close_num + 1, use_gpu=True) # 入出力が同じだと一見変な感じがするが，交差エントロピーと違ってcenterlossを使うと最初から決めていれば，モデルの出力サイズを必ずしもクラス数に一致させる必要がないからfeat_dimを任意に設定できる．\n",
    "optimizer_centloss = torch.optim.SGD(center_loss.parameters(), lr=0.05)\n",
    "\n",
    "\n",
    "class AngularPenaltySMLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, loss_type='cosface', eps=1e-7, s=None, m=None):\n",
    "        '''\n",
    "        Angular Penalty Softmax Loss\n",
    "        Three 'loss_types' available: ['arcface', 'sphereface', 'cosface']\n",
    "        These losses are described in the following papers: \n",
    "        \n",
    "        ArcFace: https://arxiv.org/abs/1801.07698\n",
    "        SphereFace: https://arxiv.org/abs/1704.08063\n",
    "        CosFace/Ad Margin: https://arxiv.org/abs/1801.05599\n",
    "        '''\n",
    "        super(AngularPenaltySMLoss, self).__init__()\n",
    "        loss_type = loss_type.lower()\n",
    "        assert loss_type in  ['arcface', 'sphereface', 'cosface']\n",
    "        if loss_type == 'arcface':\n",
    "            self.s = 64.0 if not s else s\n",
    "            self.m = 0.5 if not m else m\n",
    "        if loss_type == 'sphereface':\n",
    "            self.s = 64.0 if not s else s\n",
    "            self.m = 1.35 if not m else m\n",
    "        if loss_type == 'cosface':\n",
    "            self.s = 32.0 if not s else s\n",
    "            self.m = 0.2 if not m else m\n",
    "        self.loss_type = loss_type\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.fc = nn.Linear(in_features, out_features, bias=False)\n",
    "        self.fc.to(device)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        '''\n",
    "        input shape (N, in_features)\n",
    "        '''\n",
    "        assert len(x) == len(labels)\n",
    "        assert torch.min(labels) >= 0\n",
    "        assert torch.max(labels) < self.out_features\n",
    "        \n",
    "        for W in self.fc.parameters():\n",
    "            W = F.normalize(W, p=2, dim=1)\n",
    "\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        # logを引き算に変えて計算\n",
    "        wf = self.fc(x)\n",
    "        if self.loss_type == 'cosface':\n",
    "            numerator = self.s * (torch.diagonal(wf.transpose(0, 1)[labels]) - self.m)\n",
    "        if self.loss_type == 'arcface':\n",
    "            numerator = self.s * torch.cos(torch.acos(torch.clamp(torch.diagonal(wf.transpose(0, 1)[labels]), -1.+self.eps, 1-self.eps)) + self.m)\n",
    "        if self.loss_type == 'sphereface':\n",
    "            numerator = self.s * torch.cos(self.m * torch.acos(torch.clamp(torch.diagonal(wf.transpose(0, 1)[labels]), -1.+self.eps, 1-self.eps)))\n",
    "\n",
    "        excl = torch.cat([torch.cat((wf[i, :y], wf[i, y+1:])).unsqueeze(0) for i, y in enumerate(labels)], dim=0)\n",
    "        denominator = torch.exp(numerator) + torch.sum(torch.exp(self.s * excl), dim=1)\n",
    "        L = numerator - torch.log(denominator)\n",
    "        return -torch.mean(L)\n",
    "\n",
    "cos_loss = AngularPenaltySMLoss(close_num + 1, close_num + 1, loss_type=\"cosface\") # center_lossと同じ理由でin_featuresはクラス数でよい．\n",
    "\n",
    "\n",
    "def triple_joint_loss(output, label, alpha):\n",
    "    # alpha: hyper parameter\n",
    "    output_only_truth = []\n",
    "    for idx, x in enumerate(output):\n",
    "        x = x[labels[idx]]\n",
    "        x = torch.tensor(x).to(device)\n",
    "        output_only_truth.append([x])\n",
    "    output_only_truth = torch.tensor(output_only_truth)\n",
    "    output_only_truth = output_only_truth.float()\n",
    "    output_only_truth = output_only_truth.to(device)\n",
    "    # print(output.is_cuda, output_only_truth.is_cuda, label.is_cuda)\n",
    "\n",
    "    return softmax_loss(output, label) + alpha * center_loss(output, label)\n",
    "    # return softmax_loss(output, label) + alpha * center_loss(output, label) + cos_loss(output, label)\n",
    "    # return cos_loss(output_only_truth, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19013/3433469541.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x).to(device)\n",
      "/home/riku/workspace/InceptionTime/env/lib/python3.8/site-packages/torch/overrides.py:1534: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  result = torch_func_method(public_api, types, args, kwargs)\n",
      "/home/riku/workspace/InceptionTime/center_loss.py:45: UserWarning: This overload of addmm_ is deprecated:\n",
      "\taddmm_(Number beta, Number alpha, Tensor mat1, Tensor mat2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddmm_(Tensor mat1, Tensor mat2, *, Number beta, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1420.)\n",
      "  distmat.addmm_(1, -2, x, self.centers.t())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/`300], Step [10/46], Loss: 4.4686\n",
      "Epoch [1/`300], Step [20/46], Loss: 4.2228\n",
      "Epoch [1/`300], Step [30/46], Loss: 4.3167\n",
      "Epoch [1/`300], Step [40/46], Loss: 4.0492\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m outputs \u001b[39m=\u001b[39m model(signals)\n\u001b[1;32m     12\u001b[0m outputs \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 13\u001b[0m loss \u001b[39m=\u001b[39m triple_joint_loss(outputs, labels, alpha)\n\u001b[1;32m     14\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     15\u001b[0m optimizer_centloss\u001b[39m.\u001b[39mzero_grad()\n",
      "Cell \u001b[0;32mIn [9], line 84\u001b[0m, in \u001b[0;36mtriple_joint_loss\u001b[0;34m(output, label, alpha)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39mfor\u001b[39;00m idx, x \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(output):\n\u001b[1;32m     83\u001b[0m     x \u001b[39m=\u001b[39m x[labels[idx]]\n\u001b[0;32m---> 84\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(x)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     85\u001b[0m     output_only_truth\u001b[39m.\u001b[39mappend([x])\n\u001b[1;32m     86\u001b[0m output_only_truth \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(output_only_truth)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = InceptionTime(1, close_num + 1)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (signals, labels) in enumerate(train_loader):\n",
    "        signals = signals.float()\n",
    "        signals = signals.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(signals)\n",
    "        outputs = outputs.to(device)\n",
    "        loss = triple_joint_loss(outputs, labels, alpha)\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_centloss.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in center_loss.parameters():\n",
    "            param.grad.data *= (1./alpha)\n",
    "        optimizer.step()\n",
    "        optimizer_centloss.step()\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/`{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Confusion Matrix\n",
    "predicted_lists = np.zeros(0, dtype=np.int64)\n",
    "one_hot_labels_list = np.zeros(0, dtype=np.int64)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  n_correct = 0\n",
    "  n_samples = 0\n",
    "  softmax = nn.Softmax()\n",
    "  for i, (signals, one_hot_labels) in enumerate(test_loader):\n",
    "    signals = signals.float()\n",
    "    signals = signals.to(device)\n",
    "    one_hot_labels = one_hot_labels.to(device)\n",
    "    outputs = model(signals)\n",
    "    for j, out in enumerate(outputs):\n",
    "      outputs[j] = softmax(out)\n",
    "    _, predicted = torch.max(outputs.data, 1) # predicted per batch size\n",
    "    \"\"\"\n",
    "    Opensetのためのthreshold-softmax\n",
    "    for idx in range(len(_)):\n",
    "      if _[idx] < threshold:\n",
    "        predicted[idx] = Unknown_label # 15, 20, 25\n",
    "    \"\"\"\n",
    "    print(_, predicted, one_hot_labels)\n",
    "    n_samples += one_hot_labels.size(0) # add batch_size\n",
    "    n_correct += (predicted == one_hot_labels).sum().item()\n",
    "    \n",
    "    predicted_cp = predicted.to('cpu').detach().numpy().copy()\n",
    "    one_hot_labels_cp = one_hot_labels.to('cpu').detach().numpy().copy()\n",
    "    predicted_lists = np.concatenate([predicted_lists, predicted_cp])\n",
    "    one_hot_labels_list = np.concatenate([one_hot_labels_list, one_hot_labels_cp])\n",
    "    \n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'{n_correct} / {n_samples} = Acc: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(one_hot_labels_list, predicted_lists)\n",
    "sns.heatmap(cm, square=True, cbar=True, annot=True, cmap='Blues')\n",
    "plt.xlabel(\"Predicted Label\", fontsize=13)\n",
    "plt.ylabel(\"Ground Truth\", fontsize=13)\n",
    "fig_name = \"sklearn_confusion_matrix_{}_{}.png\".format(Unknown_label, threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f0cee1d93a3f453c87d2fc7ea34167ba7e841aa689af4edcc0624fd2f7da055"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
