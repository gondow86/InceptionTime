{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1699/1956042176.py:296: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  signals = torch.tensor(signals)\n",
      "/tmp/ipykernel_1699/1956042176.py:278: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x).to(device)\n",
      "/home/riku/workspace/InceptionTime/env/lib/python3.8/site-packages/torch/overrides.py:1534: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  result = torch_func_method(public_api, types, args, kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/`200], Step [12/34], Loss: 5.1192\n",
      "Epoch [1/`200], Step [24/34], Loss: 4.8428\n",
      "Epoch [2/`200], Step [12/34], Loss: 4.6399\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [115], line 313\u001b[0m\n\u001b[1;32m    311\u001b[0m     optimizer_centloss\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    312\u001b[0m     \u001b[39mif\u001b[39;00m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m12\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 313\u001b[0m       \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/`\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m], Step [\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mn_total_steps\u001b[39m}\u001b[39;00m\u001b[39m], Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    314\u001b[0m threshold \u001b[39m=\u001b[39m \u001b[39m0.65\u001b[39m\n\u001b[1;32m    315\u001b[0m \u001b[39m# For Confusion Matrix\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/InceptionTime/env/lib/python3.8/site-packages/fastai/torch_core.py:376\u001b[0m, in \u001b[0;36mTensorBase.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mdebug \u001b[39mand\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39m__str__\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m__repr__\u001b[39m\u001b[39m'\u001b[39m): \u001b[39mprint\u001b[39m(func, types, args, kwargs)\n\u001b[1;32m    375\u001b[0m \u001b[39mif\u001b[39;00m _torch_handled(args, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_opt, func): types \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39mTensor,)\n\u001b[0;32m--> 376\u001b[0m res \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m__torch_function__(func, types, args, ifnone(kwargs, {}))\n\u001b[1;32m    377\u001b[0m dict_objs \u001b[39m=\u001b[39m _find_args(args) \u001b[39mif\u001b[39;00m args \u001b[39melse\u001b[39;00m _find_args(\u001b[39mlist\u001b[39m(kwargs\u001b[39m.\u001b[39mvalues()))\n\u001b[1;32m    378\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mtype\u001b[39m(res),TensorBase) \u001b[39mand\u001b[39;00m dict_objs: res\u001b[39m.\u001b[39mset_meta(dict_objs[\u001b[39m0\u001b[39m],as_copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/workspace/InceptionTime/env/lib/python3.8/site-packages/torch/_tensor.py:1278\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m   1277\u001b[0m \u001b[39mwith\u001b[39;00m _C\u001b[39m.\u001b[39mDisableTorchFunction():\n\u001b[0;32m-> 1278\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1279\u001b[0m     \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   1280\u001b[0m         \u001b[39mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#|export\n",
    "from tsai.imports import *\n",
    "from tsai.models.layers import *\n",
    "from torchinfo import summary\n",
    "# !export CUDA_VISIBLE_DEVICES=0\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import signal\n",
    "# from tqdm.notebook import tqdm\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# hyper parameter\n",
    "num_classes = 30\n",
    "close_num = 14 # 14, 19, 24, 29\n",
    "batch_size = 64\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 200\n",
    "down_ratio = 8\n",
    "sequence_len = 2000 * 5 // down_ratio # default 2000Hz\n",
    "overlap = int(sequence_len * 0.3)\n",
    "threshold = 0.998\n",
    "thresholds = [0.3, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80]\n",
    "close_nums = [14, 19, 24, 29]\n",
    "alpha = 0.1\n",
    "radar_frame_list = []\n",
    "scaler = MinMaxScaler((-1, 1)) # or StandardScaler\n",
    "\n",
    "for i in range(1, num_classes + 1):\n",
    "    # wave_2d = [] # input need to be 2d?\n",
    "    file_path = \"./data/radar_%02d.csv\" % i # or ./data/radar_%02d\n",
    "    radar_frame = pd.read_csv(file_path)\n",
    "    wave = radar_frame.to_numpy().flatten()\n",
    "    wave = signal.decimate(wave, down_ratio) # down sampling\n",
    "    \n",
    "    end = len(wave)\n",
    "    n = 0\n",
    "    n_stop = sequence_len\n",
    "    wave_segments = []\n",
    "\n",
    "    while n_stop < end:\n",
    "        n_start = 0 + ((sequence_len - 1) - (overlap - 1)) * n\n",
    "        n_stop = n_start + sequence_len\n",
    "        tmp = []\n",
    "        seg = wave[n_start:n_stop].copy()\n",
    "        wave_segments.append([seg])\n",
    "        n += 1\n",
    "    \n",
    "    radar_frame_list.append(wave_segments)\n",
    "\n",
    "data_df = pd.DataFrame(radar_frame_list)\n",
    "labels = []\n",
    "for i in range(len(radar_frame_list)):\n",
    "    for j in range(len(radar_frame_list[i])):\n",
    "        if i <= close_num:\n",
    "            labels.append(i)\n",
    "        else:\n",
    "            labels.append(close_num + 1)\n",
    "\n",
    "labels_series = pd.Series(labels)\n",
    "tmp = data_df.to_numpy().flatten().copy()\n",
    "data_series = pd.Series(tmp).dropna() # remove None (keys are as they are)\n",
    "tmp = data_series.to_numpy().flatten()\n",
    "\n",
    "tmp_labels = labels_series.to_numpy().flatten()\n",
    "# len(tmp) = 5442 (30人分のデータ)\n",
    "# len(tmp[0]) = 1 (in_channelが1のため)\n",
    "# len(tmp[0][0]) = 1250 (sequence_len)\n",
    "\n",
    "for i in reversed(range(len(tmp))):\n",
    "  if len(tmp[i][0]) != sequence_len:\n",
    "    # print(i, len(tmp[i][0]))\n",
    "    tmp = np.delete(tmp, i)\n",
    "    tmp_labels = np.delete(tmp_labels, i)\n",
    "\n",
    "data_series = pd.Series(tmp)\n",
    "labels_series = pd.Series(tmp_labels)\n",
    "labels_tensor = torch.tensor(labels_series)\n",
    "# data_series.shape, labels_tensor, labels_tensor.shape\n",
    "class InceptionModule(Module):\n",
    "    def __init__(self, ni, nf, ks=40, bottleneck=True):\n",
    "        ks = [ks // (2**i) for i in range(3)]\n",
    "        ks = [k if k % 2 != 0 else k - 1 for k in ks]  # ensure odd ks\n",
    "        bottleneck = bottleneck if ni > 1 else False\n",
    "        self.bottleneck = Conv1d(ni, nf, 1, bias=False) if bottleneck else noop\n",
    "        self.convs = nn.ModuleList([Conv1d(nf if bottleneck else ni, nf, k, bias=False) for k in ks])\n",
    "        self.maxconvpool = nn.Sequential(*[nn.MaxPool1d(3, stride=1, padding=1), Conv1d(ni, nf, 1, bias=False)])\n",
    "        self.concat = Concat()\n",
    "        self.bn = BN1d(nf * 4)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_tensor = x\n",
    "        x = self.bottleneck(input_tensor)\n",
    "        x = self.concat([l(x) for l in self.convs] + [self.maxconvpool(input_tensor)])\n",
    "        return self.act(self.bn(x))\n",
    "\n",
    "\n",
    "@delegates(InceptionModule.__init__)\n",
    "class InceptionBlock(Module):\n",
    "    def __init__(self, ni, nf=32, residual=True, depth=6, **kwargs):\n",
    "        self.residual, self.depth = residual, depth\n",
    "        self.inception, self.shortcut = nn.ModuleList(), nn.ModuleList()\n",
    "        for d in range(depth):\n",
    "            self.inception.append(InceptionModule(ni if d == 0 else nf * 4, nf, **kwargs))\n",
    "            if self.residual and d % 3 == 2: \n",
    "                n_in, n_out = ni if d == 2 else nf * 4, nf * 4\n",
    "                self.shortcut.append(BN1d(n_in) if n_in == n_out else ConvBlock(n_in, n_out, 1, act=None))\n",
    "        self.add = Add()\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        for d, l in enumerate(range(self.depth)):\n",
    "            x = self.inception[d](x)\n",
    "            if self.residual and d % 3 == 2: res = x = self.act(self.add(x, self.shortcut[d//3](res)))\n",
    "        return x\n",
    "\n",
    "    \n",
    "@delegates(InceptionModule.__init__)\n",
    "class InceptionTime(Module):\n",
    "    def __init__(self, c_in, c_out, seq_len=None, nf=32, nb_filters=None, **kwargs):\n",
    "        nf = ifnone(nf, nb_filters) # for compatibility\n",
    "        self.inceptionblock = InceptionBlock(c_in, nf, **kwargs) # c_in is input channel num of conv1d\n",
    "        self.gap = GAP1d(1)\n",
    "        self.fc = nn.Linear(nf * 4, c_out) # c_out is 1d output size \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.inceptionblock(x)\n",
    "        x = self.gap(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "# from tsai.models.utils import count_parameters\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset, labels, root_dir, transform=None) -> None:\n",
    "        # super().__init__()\n",
    "        self.radar_heartbeat = dataset\n",
    "        self.labels = labels\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "          idx = idx.tolist()\n",
    "        \n",
    "        onehot_label = torch.eye(num_classes)[self.labels[idx] - 1] # one hot encodingは不要らしい　精度悪い場合試す必要あり\n",
    "        # one_hot = torch.nn.functional.one_hot(self.labels, num_classes=num_classes)\n",
    "        return torch.tensor(self.radar_heartbeat[idx]), self.labels[idx] # labels is already tensor (converted in preparation phase)\n",
    "        # return torch.tensor(self.radar_heartbeat[idx]), onehot_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.radar_heartbeat)\n",
    "\n",
    "\n",
    "dataset = MyDataset(data_series, labels_tensor, \"./data/\", transform=transforms.ToTensor())\n",
    "\n",
    "# indices1 = np.arange(4473)\n",
    "# dataset1 = torch.utils.data.Subset(dataset, indices1) # 0-24\n",
    "# indices2 = np.arange(4474, len(dataset))\n",
    "# dataset2 = torch.utils.data.Subset(dataset, indices2) # 25-30\n",
    "# Unknown_label = close_num + 1\n",
    "# indices1 = np.arange(3570)\n",
    "# dataset1 = torch.utils.data.Subset(dataset, indices1) # 0-19\n",
    "# indices2 = np.arange(3571, len(dataset))\n",
    "# dataset2 = torch.utils.data.Subset(dataset, indices2) # 20-30\n",
    "# Unknown_label = close_num + 1\n",
    "indices1 = np.arange(2691)\n",
    "dataset1 = torch.utils.data.Subset(dataset, indices1) # 0-14\n",
    "indices2 = np.arange(2692, len(dataset))\n",
    "dataset2 = torch.utils.data.Subset(dataset, indices2) # 15-29\n",
    "Unknown_label = close_num + 1\n",
    "train_size1 = int(0.80 * len(dataset1))\n",
    "test_size1 = len(dataset1) - train_size1\n",
    "\n",
    "train_size2 = int(0.80 * len(dataset2))\n",
    "test_size2 = len(dataset2) - train_size2\n",
    "\n",
    "open_train_set, test_set1 = torch.utils.data.random_split(dataset1, [train_size1, test_size1])\n",
    "train_set2, test_set2 = torch.utils.data.random_split(dataset2, [train_size2, test_size2])\n",
    "\n",
    "indices = np.arange(len(test_set2))\n",
    "test_set2 = torch.utils.data.Subset(test_set2, indices[:])\n",
    "open_test_set = torch.utils.data.ConcatDataset([test_set1, test_set2])\n",
    "len(open_train_set), len(open_test_set)\n",
    "train_loader = DataLoader(dataset=open_train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=open_test_set, batch_size=batch_size, shuffle=False) # テストでシャッフルしても同じ\n",
    "from torch import log\n",
    "softmax = nn.Softmax()\n",
    "\n",
    "def softmax_loss(outputs, labels):\n",
    "    loss = 0\n",
    "    batch_size = len(labels)\n",
    "    logsoftmax_out = log(softmax(outputs))\n",
    "    for idx in range(batch_size):\n",
    "        loss += 1.0 - logsoftmax_out[idx][labels[idx]]\n",
    "    \n",
    "    return loss / batch_size\n",
    "\n",
    "\n",
    "from center_loss import CenterLoss\n",
    "center_loss = CenterLoss(num_classes=close_num + 1, feat_dim=close_num + 1, use_gpu=True) # 入出力が同じだと一見変な感じがするが，交差エントロピーと違ってcenterlossを使うと最初から決めていれば，モデルの出力サイズを必ずしもクラス数に一致させる必要がないからfeat_dimを任意に設定できる．\n",
    "optimizer_centloss = torch.optim.SGD(center_loss.parameters(), lr=0.05)\n",
    "\n",
    "\n",
    "class AngularPenaltySMLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, loss_type='cosface', eps=1e-7, s=None, m=None):\n",
    "        '''\n",
    "        Angular Penalty Softmax Loss\n",
    "        Three 'loss_types' available: ['arcface', 'sphereface', 'cosface']\n",
    "        These losses are described in the following papers: \n",
    "        \n",
    "        ArcFace: https://arxiv.org/abs/1801.07698\n",
    "        SphereFace: https://arxiv.org/abs/1704.08063\n",
    "        CosFace/Ad Margin: https://arxiv.org/abs/1801.05599\n",
    "        '''\n",
    "        super(AngularPenaltySMLoss, self).__init__()\n",
    "        loss_type = loss_type.lower()\n",
    "        assert loss_type in  ['arcface', 'sphereface', 'cosface']\n",
    "        if loss_type == 'arcface':\n",
    "            self.s = 64.0 if not s else s\n",
    "            self.m = 0.5 if not m else m\n",
    "        if loss_type == 'sphereface':\n",
    "            self.s = 64.0 if not s else s\n",
    "            self.m = 1.35 if not m else m\n",
    "        if loss_type == 'cosface':\n",
    "            self.s = 32.0 if not s else s\n",
    "            self.m = 0.2 if not m else m\n",
    "        self.loss_type = loss_type\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.fc = nn.Linear(in_features, out_features, bias=False)\n",
    "        self.fc.to(device)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        '''\n",
    "        input shape (N, in_features)\n",
    "        '''\n",
    "        assert len(x) == len(labels)\n",
    "        assert torch.min(labels) >= 0\n",
    "        assert torch.max(labels) < self.out_features\n",
    "        \n",
    "        for W in self.fc.parameters():\n",
    "            W = F.normalize(W, p=2, dim=1)\n",
    "\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        # logを引き算に変えて計算\n",
    "        wf = self.fc(x)\n",
    "        if self.loss_type == 'cosface':\n",
    "            numerator = self.s * (torch.diagonal(wf.transpose(0, 1)[labels]) - self.m)\n",
    "        if self.loss_type == 'arcface':\n",
    "            numerator = self.s * torch.cos(torch.acos(torch.clamp(torch.diagonal(wf.transpose(0, 1)[labels]), -1.+self.eps, 1-self.eps)) + self.m)\n",
    "        if self.loss_type == 'sphereface':\n",
    "            numerator = self.s * torch.cos(self.m * torch.acos(torch.clamp(torch.diagonal(wf.transpose(0, 1)[labels]), -1.+self.eps, 1-self.eps)))\n",
    "\n",
    "        excl = torch.cat([torch.cat((wf[i, :y], wf[i, y+1:])).unsqueeze(0) for i, y in enumerate(labels)], dim=0)\n",
    "        denominator = torch.exp(numerator) + torch.sum(torch.exp(self.s * excl), dim=1)\n",
    "        L = numerator - torch.log(denominator)\n",
    "        return -torch.mean(L)\n",
    "\n",
    "cos_loss = AngularPenaltySMLoss(close_num + 1, close_num + 1, loss_type=\"cosface\") # center_lossと同じ理由でin_featuresはクラス数でよい．\n",
    "\n",
    "\n",
    "def triple_joint_loss(output, label, alpha):\n",
    "    # alpha: hyper parameter\n",
    "    output_only_truth = []\n",
    "    for idx, x in enumerate(output):\n",
    "        x = x[labels[idx]]\n",
    "        x = torch.tensor(x).to(device)\n",
    "        output_only_truth.append([x])\n",
    "    output_only_truth = torch.tensor(output_only_truth)\n",
    "    output_only_truth = output_only_truth.float()\n",
    "    output_only_truth = output_only_truth.to(device)\n",
    "    # print(output.is_cuda, output_only_truth.is_cuda, label.is_cuda)\n",
    "\n",
    "    return softmax_loss(output, label) + alpha * center_loss(output, label)\n",
    "model = InceptionTime(1, close_num + 1) # 0-?+Unknownを出力\n",
    "model.to(device)\n",
    "# if device == 'cuda':\n",
    "    # model = torch.nn.DataParallel(model)\n",
    "    # torch.backends.cudnn.benchmark = True\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "  for i, (signals, labels) in enumerate(train_loader):\n",
    "    signals = torch.tensor(signals)\n",
    "    signals = signals.float()\n",
    "    signals = signals.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # print(signals.size())\n",
    "    outputs = model(signals)\n",
    "    outputs = outputs.to(device)\n",
    "    # print(outputs)\n",
    "    loss = triple_joint_loss(outputs, labels, alpha) # will check the shapes of outputs and labels\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    optimizer_centloss.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer_centloss.step()\n",
    "    if (i + 1) % 12 == 0:\n",
    "      print(f'Epoch [{epoch+1}/`{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "threshold = 0.65\n",
    "# For Confusion Matrix\n",
    "predicted_lists = np.zeros(0, dtype=np.int64)\n",
    "one_hot_labels_list = np.zeros(0, dtype=np.int64)\n",
    "\n",
    "with torch.no_grad():\n",
    "  n_correct = 0\n",
    "  n_samples = 0\n",
    "  softmax = nn.Softmax()\n",
    "  for i, (signals, one_hot_labels) in enumerate(test_loader):\n",
    "    signals = torch.tensor(signals)\n",
    "    signals = signals.float()\n",
    "    signals = signals.to(device)\n",
    "    one_hot_labels = one_hot_labels.to(device)\n",
    "    # print(len(one_hot_labels))\n",
    "    outputs = model(signals)\n",
    "    for j, out in enumerate(outputs):\n",
    "      outputs[j] = softmax(out)\n",
    "\n",
    "    _, predicted = torch.max(outputs.data, 1) # predicted per batch size\n",
    "    \n",
    "    \n",
    "    for idx in range(len(_)):\n",
    "      if _[idx] < threshold:\n",
    "        predicted[idx] = Unknown_label # 15, 20, 25\n",
    "    print(_, predicted, one_hot_labels)\n",
    "\n",
    "    n_samples += one_hot_labels.size(0) # add batch_size\n",
    "    n_correct += (predicted == one_hot_labels).sum().item()\n",
    "    \n",
    "    predicted_cp = predicted.to('cpu').detach().numpy().copy()\n",
    "    one_hot_labels_cp = one_hot_labels.to('cpu').detach().numpy().copy()\n",
    "    predicted_lists = np.concatenate([predicted_lists, predicted_cp])\n",
    "    one_hot_labels_list = np.concatenate([one_hot_labels_list, one_hot_labels_cp])\n",
    "    \n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'{n_correct} / {n_samples} = Acc: {acc} %')\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# cm = confusion_matrix(one_hot_labels_list, predicted_lists)\n",
    "# sns.heatmap(cm, square=True, cbar=True, annot=True, cmap='Blues')\n",
    "# plt.xlabel(\"Predicted Label\", fontsize=13)\n",
    "# plt.ylabel(\"Ground Truth\", fontsize=13)\n",
    "# fig_name = \"sklearn_confusion_matrix_{}_{}.png\".format(Unknown_label, threshold)\n",
    "# plt.savefig(\"./figure/\" + fig_name)\n",
    "# a = np.array([55.35055350553505, 61.99261992619926, 64.11439114391143, 70.75645756457564, 74.6309963099631, 73.5239852398524, 77.58302583025831, 84.5940959409594, 81.91881918819188, 90.59040590405904, 88.46863468634686])\n",
    "# b = np.array([55.71955719557196, 57.564575645756456, 61.62361623616236, 70.9409594095941, 71.58671586715867, 76.56826568265683, 75.27675276752767, 76.19926199261992, 81.2730627306273, 84.96309963099631, 88.00738007380073])\n",
    "# c = np.array([51.93726937269373, 54.7970479704797, 58.025830258302584, 62.08487084870849, 66.32841328413284, 71.03321033210332, 74.72324723247233, 78.50553505535055, 81.91881918819188, 84.77859778597787, 87.82287822878229])\n",
    "# d = np.array([52.859778597785976, 56.91881918819188, 61.254612546125465, 66.14391143911439, 70.84870848708488, 75.46125461254613, 78.87453874538745, 82.10332103321034, 85.05535055350553, 87.63837638376384, 90.59040590405904])\n",
    "# e = np.array([52.5830258302583, 56.18081180811808, 60.42435424354244, 64.11439114391143, 67.98892988929889, 72.32472324723247, 75.09225092250922, 78.78228782287823, 82.380073800738, 85.23985239852398, 88.09963099630997])\n",
    "# ave14 = np.mean([a, b, c, d, e], axis=0)\n",
    "\n",
    "# a1 = np.array([68.88273314866113, 73.40720221606648, 77.65466297322253, 78.67036011080333, 78.76269621421976, 84.57987072945522, 83.28716528162512, 87.44228993536473, 85.96491228070175, 88.9196675900277, 88.73499538319483])\n",
    "# b1 = np.array([69.3444136657433, 71.46814404432133, 76.08494921514313, 78.02400738688827, 78.30101569713759, 76.5466297322253, 83.10249307479225, 83.28716528162512, 87.07294552169898, 87.6269621421976, 90.2123730378578])\n",
    "# c1 = np.array([69.43674976915975, 71.83748845798708, 74.33056325023084, 76.36195752539243, 78.20867959372114, 81.4404432132964, 83.10249307479225, 85.68790397045245, 86.79593721144968, 89.10433979686057, 91.68975069252078])\n",
    "# d1 = np.array([68.69806094182826, 70.3601108033241, 72.11449676823638, 74.42289935364728, 75.53093259464451, 76.73130193905817, 78.02400738688827, 78.76269621421976, 80.33240997229917, 81.80978762696215, 84.11819021237304])\n",
    "# e1 = np.array([69.71375807940905, 72.29916897506925, 74.9769159741459, 77.93167128347184, 79.87072945521699, 81.4404432132964, 82.82548476454294, 84.57987072945522, 86.0572483841182, 87.44228993536473, 88.73499538319483])\n",
    "\n",
    "# ave19 = np.mean([a1, b1, c1, d1, e1], axis=0)\n",
    "\n",
    "# a2 = np.array([88.36565096952909, 91.87442289935365, 91.59741458910435, 93.62880886426593, 92.89012003693445, 93.90581717451524, 96.67590027700831, 95.10618651892891, 98.33795013850416, 95.38319482917821, 97.78393351800554])\n",
    "# b2 = np.array([87.90397045244691, 88.82733148661127, 91.59741458910435, 88.6426592797784, 91.2280701754386, 94.73684210526316, 94.73684210526316, 93.72114496768236, 95.93721144967682, 95.66020313942752, 97.04524469067405])\n",
    "# c2 = np.array([85.87257617728532, 87.53462603878117, 89.47368421052632, 91.59741458910435, 92.05909510618652, 93.25946445060019, 93.8134810710988, 95.01385041551247, 95.8448753462604, 96.67590027700831, 97.96860572483841])\n",
    "# d2 = np.array([86.88827331486611, 88.18097876269621, 89.38134810710989, 90.39704524469067, 91.5050784856879, 92.89012003693445, 94.45983379501385, 95.29085872576178, 96.02954755309327, 96.86057248384118, 97.4145891043398])\n",
    "# e2 = np.array([86.0572483841182, 87.16528162511543, 88.6426592797784, 91.13573407202216, 92.98245614035088, 94.73684210526316, 96.21421975992614, 97.4145891043398, 97.87626962142198, 98.15327793167128, 98.52262234533703])\n",
    "\n",
    "# ave24 = np.mean([a2, b2, c2, d2, e2], axis=0)\n",
    "\n",
    "# x = np.linspace(0.30, 0.80, 11)\n",
    "# y1 = ave14\n",
    "# y2 = ave19\n",
    "# y3 = ave24\n",
    "# y0 = [99.08, 99.08, 99.08, 99.08, 99.08, 99.08, 99.08, 99.08, 99.08, 99.08, 99.08]\n",
    "# plt.ylim(55, 100)\n",
    "# plt.xlabel(\"Threshold\")\n",
    "# plt.ylabel(\"Accuracy [%]\")\n",
    "# # plt.plot(x, y0, label=\"Openness=0, (threshold=0)\")\n",
    "# plt.plot(x, y3, marker=\"o\", label=\"Openness=0.087\")\n",
    "# plt.plot(x, y2, marker=\"*\", label=\"Openness=0.184\")\n",
    "# plt.plot(x, y1, marker=\"x\", label=\"Openness=0.293\")\n",
    "# plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f0cee1d93a3f453c87d2fc7ea34167ba7e841aa689af4edcc0624fd2f7da055"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
